---
layout: default
---

# 嵌入式AI简报 (2022-07-01)：ARM新架构/MegPeak/FlashAttention/ECCV2022/YOLOv6  


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：【新闻】ARM新架构的大/中/小核深度解读、高通发布AI软件栈推动OEM厂商和AI开发者、苹果发布5nm M2处理器8核CPU+10核GPU；【论文】优化内存读写效率的FlashAttention算法、美团发布YOLOv6精度超YOLOv5和YOLOX、移动平台1ms的MobileOne骨干，苹果手机实时推理模型EfficientFormer；【开源】高性能计算的辅助工具MegPeak，TensorFlow2.9发布亮点性能再提升、OpenPPL在RISC-V的最新进展、Paddle2.9发布高复用算子库PHI；【博文】Tenstorrent芯片架构浅谈AI的硬件底座技术特点、MegEngine Inference 卷积优化之 Im2col 和 winograd 优化解读、最后是来自璧仞的分享之《Pluto助力MLIR编译器的多面体优化》。


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：


- ASML：表示摩尔定律可延续，当前台积电的技术优势，可在技术创新上将芯片的制程再推进至少1nm，且光刻系统分辨率的改进（预计每6年左右缩小2倍）和边缘放置误差（EPE）对精度的衡量也将进一步实现缩小晶片尺寸；
- 台积电：2nm N2技术计划2025年生产，让设计人员在同功率和晶体管数量下将性能提升10%~15%，或相同频率和复杂度下将功耗降低25%~30%。2nm N2技术将广泛使用EUV光刻，并引入了GAAFET（台积电称之为纳米片晶体管）以及背面供电；预计台南的生产中心再建四座覆盖 Apple SoC、 A 系列芯片的价值100亿美元的3nm制造工厂；
Imagination：推出首款32位实时嵌入式IMG RISC-V CPU-RTXM-2200，128KB L1 I-Cache和D-Cache，支持fp32/bf16，用于低成本AI部署；GPU获得ADAS和HMI应用的ISO 26262功能安全认证；
- 三星：李在镕与ASML达成协议，引进ASML今年生产的EUV光刻设备和计划明年推出的High-NA EUV光刻设备。预计三星将从2024年开始实际使用High-NA EUV光刻设备，总得来说，三星电子已获得计划于今年生产的55台EUV光刻设备中的18台；宣布基于 GAA （全环栅晶体管） 量产3nm 工艺，或将是世界上第一个迈入 GAA 结构的晶圆厂；目前在开发下一代旗舰SoC Exynos 2300，型号S5E9935代号为Quadra，采用三星3nm GAA工艺，CPU为ARM最新架构，GPU为AMD最新的Radeon GPU，或将于2023年上半年Galaxy S23系列首发；
- 龙芯：龙芯中科登陆科创板，08年成立，主营业务为处理器及配套芯片的研制、销售及服务。其下一代3A6000预计2023年发布，采用12nm并有大幅度架构升级，单核SPEC CPU 2006定点/浮点base分值（GCC）从26/28分提高到35/45分，内存双通道DDR4的Stream带宽(峰值51.2GBps)也将从25GBps提高到38GBps；
- RISC-V：架构改进，宣布2022年的首批四项规格和扩展的批准：高效跟踪（E-Trace）、主管二进制接口（SBI）、统一可扩展固件接口（UEFI）规格，及Zmmul纯乘法扩展有利于简单的FPGA软核；
- 联发科：发布旗下首款5G毫米波移动平台——天玑1050，6nm，CPU为A78@2.5GHz\*2+A55\*6，GPU为mali-G610，APU为APU550提升AI相机功能等；CounterPoint公布了2022Q1全球智能手机SoC市场统计报告，联发科以天玑系列，以38％的市场份额再拿下第一。已连续七个季度全球第一，目前已覆盖高，中低端（天玑700/900）。按出货量算，高通30%位居第二，苹果15％，紫光展锐11％，三星5％，海思1％；
- AMD：支持芯片定制，向第三方Chiplet打开大门。允许第三方制造AI加速器或汽车等其他芯片的客户，在封装中实现多个裸片（也称为chiplet或compute tiles ），与 x86 CPU 和 GPU ，AI加速器一起包含在 其2D 或 3D 封装中；
- 英特尔：表示若美国国会不能批准“芯片法案”中承诺的520亿美元的芯片制造政府补助，英特尔可能会在欧洲而不是美国扩大芯片生产。由于缺乏政府资金，将推迟在俄亥俄州价值200亿美元的新工厂奠基仪式。目前已计划投入约350亿美元来扩大其在欧盟的生产，包含德国新建价值180亿美元的设施；
- 恩智浦：关闭中国区APS（Advanced Power System 先进电源系统）研发部门，后者专注于消费电子类Power Delivery产品；
- 高通 & Arm：高通CEO安蒙表示有意联合其它公司，在Arm IPO时入股或收购。英国方面当被问及“政府是否会利用国家安全权力，迫使软银旗下的Arm于伦敦上市”时，相关发言人称不知道有政府任何相关计划这样做。此前英国金融时报曾报道称，政府官员们正在考虑迫使ARM在伦敦上市；
- 高通：第2代骁龙8今年年底登场，大核变多，将采用台积电4nm工艺，且X3\*1+A720\*2+A710\*2+3的八核心架构设计，骁龙8+为1\*X2+3\*AA710+4的架构，2代的GPU为Adreno 740；
- 苹果：目前其5G基带芯片开发可能已失败，因此高通将继续成为2023年新iPhone的5G芯片独家供应商，高通2023年下半年到2024年上半年的收入可能超过预期，份额为100%；
- MIT：MIT工程师构建类似乐高的 AI 芯片。这种芯片构件可使设备保持最新状态，并减少电子浪费，其特点是用光而非物理线来传输信息，可按需添加任意数量的计算层和光、压力甚至气味传感器；
- Android：谷歌推出 Android 13 的第三个测试版本，达到了“平台稳定”，接下来打磨细节。


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [深度解读ARM新架构：X3大核更强更大、中核更省，小核原地踏步，旗舰GPU支持硬件光追 | 三易生活](https://mp.weixin.qq.com/s/3_R5ULRMOe6fr_bXiDM7qA)  
摘要：全新的Cortex-X3大核。在相同制程&&主频&&缓存下，X3比X2性能提升11%。比前代大10倍的L0 BTB（分支目标缓冲区）和大50%的L1 BTB，X3的分支预测性能大幅提高，其分支预测延迟降低了12.2%，预测错误率降低了6%，同时减少了3%的前段停顿。由于分支预测性能大为提升，因此Cortex-X3的mop（微操作）缓存现在可以做得更小，同时流水线长度也进一步下降。  
改进型的Cortex-A715中核，在相同制程&&主频&&缓存下，新中核性能相比老设计仅5%提升。指令缓存提取宽度现在从5增加到了6、算术逻辑单元从4个增加到了6个，乱序窗口也进一步增大。  
新架构里Cortex-A510小核的改进就比较微妙了。一方面，新小核仍叫做“Cortex-A510”，名字没变。但另一方面，要说它完全没改又不太对，因为新版的小核功耗比老版本下降了5%，更为重要的是，其此次加上了对32位应用的可选兼容性。  
发布的GPU方案分为三档：顶配的Immortalis（不朽）-G715、中配的Mali-G715，及入门级的Mali-G615。Mali-G710和Mali-G610相比，新架构有15%效率提升，还支持VRS可变着色率技术可在部分高刷游戏中降低渲染负载、对XR应用也有意义。相比现有Mali-G710，新款GPU在基础架构上将FMA乘加单元的数量翻了一倍。旗舰Immortalis-G715首次引入了硬件光线追踪单元，但这次新GPU都是共享的相同架构设计（只是Immortalis-G715内部多了硬件光追电路），主要区别还是在于核心数量上做了明确限制。  
- [高通AI软件栈：推动OEM厂商和开发者的AI开发，已商用上市 | 高通中国](https://mp.weixin.qq.com/s/e10-dPe0OqZBNGpP8AD8Hg)  
摘要：该AI软件栈是面向OEM厂商和开发者的一套完整的AI解决方案，覆盖智能手机、汽车、XR、计算、物联网和云平台。  
AI软件栈支持包括TensorFlow、PyTorch和ONNX在内的不同AI框架与主流runtimes，以及开发者库与服务、系统软件、工具和编译器，AI软件栈产品组合还支持一系列工具套件，包括高通AI模型增效工具包（AIMET）、AI开发图形用户界面（GUI）、用于增强量化与优化的模型分析器以及神经网络架构搜索（NAS）。  
2021骁龙技术峰会上，高通和Google Cloud宣布将Google Cloud Vertex AI NAS集成至高通神经网络处理SDK，赋能OEM厂商和生态系统打造高效的边缘侧体验。其SDK仍然是OEM厂商和开发者在高通技术公司各类产品上运行神经网络的关键。更多见：https://www.qualcomm.com/products/technology/artificial-intelligence/ai-stack  
- [苹果M2处理器发布：第二代5nm工艺 8核CPU+10核GPU | 安兔兔](https://mp.weixin.qq.com/s/kb13gKdbzuj64KG-lmVtcA)  
摘要：CPU方面，依然是8核心，4大4小，其中4个性能核心还是超宽执行架构，每核心为192KB指令缓存、128KB数据缓存，共享16MB缓存。4个能效核心，同样是宽执行架构，每核心128KB指令缓存、64KB数据缓存，共享4MB缓存，官方数据显示CPU速度整体提高18%。  
GPU集成10个核心，相比M1增加两个，速度提高35%，集成多达24GB LPDDR5统一内存，位宽128-bit，带宽超过100GB/s，比M1增加50％。得益于更大的缓存和更高的内存带宽，M2在同等功耗水平下的图形性能较M1提升最多达到25%，在最高功耗水平下的性能较M1芯片提升更可达35%。  
下一代神经引擎，每秒可以进行最多达15.8万亿次运算，支持8K H.264和HEVC视频的媒体引擎，能够同时播放多个4K和8K视频流，以及外接6K显示屏。  
- [ECCV 2022｜移动智能摄影与成像竞赛通道已开启 | 商汤学术](http://mipi-challenge.org/)  
摘要：随着移动平台上计算摄影和成像需求的增加，在相机系统中开发和集成先进的图像传感器与新颖的算法越来越普及。然而，缺乏高质量的研究数据以及产业界与学术界之间深入交流意见的机会，制约了移动智能摄影与成像的发展。  
为此，新加坡南洋理工大学S-Lab、商汤科技、上海人工智能实验室联合ECCV 2022举办题为移动智能摄影与成像（MIPI）的Workshop。MIPI以新型图像传感器和成像算法为核心，从产业界与学术界的视角讨论移动智能摄影与成像的发展。具体内容包括：竞赛、论文征集、主题报告，目前，MIPI Workshop&Challenge已上线（http://mipi-challenge.org/），欢迎感兴趣的小伙伴踊跃参与！


## 论文  


- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | 机器之心]()  
论文：https://arxiv.org/pdf/2205.14135.pdf  
摘要：通过减少 GPU 内存读取 / 写入，FlashAttention 的运行速度比 PyTorch 标准注意力快 2-4 倍，所需内存减少 5-20 倍。作者研究认为：应让注意力算法具有 IO 感知——即考虑显存级间的读写。现代 GPU 计算速度超过了内存速度，transformer 中的大多数操作都被内存访问所阻塞。IO 感知算法对于类似的内存绑定操作至关重要，这种重要性体现在当读写数据占据很大运行时——例如数据库连接、图像处理、数值线性代数等。然而，用于深度学习的常见 Python 接口，如 PyTorch 和 Tensorflow，不允许对内存访问进行细粒度控制。  
该研究提出了一种新的注意力算法 FlashAttention，它可以使用更少的内存访问来计算精确的注意力。FlashAttention 旨在避免从 HBM（High Bandwidth Memory）中读取和写入注意力矩阵。这需要做到：(i) 在不访问整个输入的情况下计算 softmax reduction；(ii) 在后向传播中不能存储中间注意力矩阵。  
作者通过 CUDA 实现了细粒度内存访问以实现 FlashAttention ，并将所有计算融合到一个 GPU 内核中。即使在需要重新计算导致 FLOPs 增加，但其运行速度更快，且使用更少的内存（序列长度线性），反而整体变快。归根结底是为大大减少了 HBM 访问量。  
- [meituan/YOLOv6: YOLOv6——精度与速度远超 YOLOv5 和 YOLOX 的新框架 | 美团技术团队](https://mp.weixin.qq.com/s/RrQCP4pTSwpTmSgvly9evg)  
代码：https://github.com/meituan/YOLOv6  
摘要：本文介绍了美团视觉智能部在目标检测框架方面的优化及实践经验，针对 YOLO 系列框架，在训练策略、主干网络、多尺度特征融合、检测头等方面进行了思考和优化，设计了新的检测框架-YOLOv6，初衷来自于解决工业应用落地时所遇到的实际问题：
    1. 更高效的 Backbone 和 Neck ：受到硬件感知神经网络设计思想的启发，基于 RepVGG style 设计了可重参数化（其结构在训练时有多分支拓扑，实际部署时可融合为单个 3x3 卷积）、更高效的骨干网络 EfficientRep Backbone 和 Rep-PAN Neck；
    2. 优化设计了更简洁有效的 Efficient Decoupled Head，维持精度同时，降低了一般解耦头带来的额外延时开销；
    3. 训练策略：采用Anchor-free 无锚范式，同时辅以 SimOTA 标签分配策略以及 SIoU 边界框回归损失来进一步提高检测精度。
在工业界常用的尺寸模型中：YOLOv6-nano 在 COCO 上精度可达 35.0% AP，在 T4 上推理速度可达 1242 FPS；YOLOv6-s 在 COCO 上精度可达 43.1% AP，在 T4 上推理速度可达 520 FPS。在部署方面，YOLOv6 支持 GPU（TensorRT）、CPU（OPENVINO）、ARM（MNN、TNN、NCNN）等不同平台的部署，极大地简化工程部署时的适配工作。  
- [MobileOne: 移动端仅需1ms的高性能骨干，你值得拥有 | AIWalker](https://mp.weixin.qq.com/s/crsRcY7dm6HJSd-QjcoUYg)  
摘要：MobileOne 是由Apple公司提出的一种基于iPhone12优化的超轻量型架构，在ImageNet数据集上以<1ms的速度取得了75.9%的Top1精度。  
其设计思路MobileOne ≈ MobileNetV1 + RepVGG + 训练Trick。作者以iPhone12平台为基准，从不同维度进行了"瓶颈"分析：发现参数多推理也可以跑得快，计算量大FLOPs也可以跑得快，那么换句话说，在移动端，延迟与FLOPs和参数量的相关性较弱，而在PC-CPU端，该相关性进一步弱化。  
性能上，对激活函数、block结构分别选择ReLU和SE模块，选择RELU是因为其他激活函数慢，选择SE是因为其单分支结构也更快。基于上述分析，MobileOne的核心模块基于MobileNetV1而设计，同时吸收了重参数思想。  
训练上，因小模型要更少正则，作者提出了Annealing的正则调整机制(可带来0.5%指标提升)；引入渐进式学习机制(可带来0.4%指标提升)；最后，作者还采用EMA机制，最终MobileOne-S2模型达到了77.4%的指标。  
- [EfficientFormer：苹果手机实时推理的Transformer模型，登顶轻量化Backbone之巅 | 集智书童](https://mp.weixin.qq.com/s/Ib6ckyjsDyafiQZKWBF8UQ)  
摘要：Vision Transformers由于大量的参数和模型设计，ViT常比轻量网络慢几倍。因此，应用部署尤其是移动端挑战大。通常大多通过网络架构搜索或与 MobileNet Block 的混合设计来降低计算复杂度，但推理速度仍然不能令人满意。这就引出了一个重要的问题：Transformer 能否在获得高性能的同时运行得像 MobileNet 一样快？  
首先重新审视基于 ViT 的模型中使用的网络架构和 ViT 算子，并确定其低效的设计。然后引入了一个维度一致的纯 Transformer （没有 MobileNet Block）作为设计范式。最后，执行延迟驱动的瘦身以获得一系列最终模型，称为 EfficientFormer。  
实验表明 EfficientFormer 在移动设备上的性能和速度方面具有优势。L1版本 在 ImageNet-1K 上实现了 79.2% 的 Top-1 准确率，在 iPhone 12（使用 CoreML 编译）上只有 1.6 ms 的推理延迟，甚至比 MobileNetV2（1.7 ms，71.8% Top-1)，EfficientFormer-L7 获得了 83.3% 的准确率，延迟仅为 7.0 ms。EfficientFormer证明，正确设计的 Transformer 可以在移动设备上达到极低的延迟，同时保持高性能。  


## 开源项目


- [MegEngine/MegPeak：MegPeak——让你更懂你的处理器：RooflineModel/最优指令组合/探索优化空间 | MegEngine](https://mp.weixin.qq.com/s/u3wL2XJlRvPFKFZjwSQ6OQ)  
摘要：MegPeak 作为一个进行高性能计算的辅助工具，能够使得开发人员轻松获得目标处理器的内在的详细信息，辅助进行对代码的性能评估，以及优化方法设计。通过 MegPeak 用户可以测试目标处理器：指令的峰值带宽、指令延迟、内存峰值带宽、任意指令组合峰值带宽。  
虽然上面的部分信息可以通过芯片的数据手册查询相关数据，然后结合理论计算得到，但是很多情况下无法获取目标处理器详尽的性能文档，另外通过 MegPeak 进行测量更直接和准确，并且可以测试特定指令组合的峰值带宽。  
- [tensorflow/tensorflow：TensorFlow 2.9 新增功能 | TensorFlow](https://mp.weixin.qq.com/s/anCKHsOyw1eCw_SnTufVSw)  
摘要：TensorFlow 2.9 已经发布有一段时间了，亮点包括 oneDNN 性能改进，以及新发布的 DTensor（DTensor 是一种新的模型分布 API，可将数据并行无缝迁移至模型并行）。  
我们还改进了核心库，包括 Eigen 和 tf.function 统一、确定性行为以及对 Windows WSL2 的新支持。此外，我们还针对 tf.function retracing 和 Keras 优化器发布了新的实验性 API。提高 CPU 性能：默认启用 oneDNN，我们与 Intel 合作，将 oneDNN 性能库与 TensorFlow 进行集成，以便在 Intel CPU 上实现更好的性能。自 TensorFlow 2.5 版本发布以来，TensorFlow 便增加了对 oneDNN 的实验性支持，实现了高达 4 倍的性能提升。为 Linux x86 软件包、具有神经网络硬件功能（如 AVX512_VNNI、AVX512_BF16、AMX 等）的 CPU，以及 Intel Cascade Lake 和其他新版 CPU 默认启用 oneDNN 优化。  
- [google/jax：谷歌官方回应：我们没有放弃TensorFlow，未来与JAX并肩发展 | 机器之心](https://mp.weixin.qq.com/s/SGVThtwKj2jl3YFRuqzCfw)  
摘要：最近大家也看到不少有关谷歌的一些部门更倾向于JAX，而非TF，谷歌发声：TensorFlow 不是谷歌的一枚「弃子」，将会继续开发。我们的愿景是创建一个有凝聚力的生态系统，研究人员和工程师可以利用系统组件进行研究，而不管它们起源于哪个框架。我们已经在 JAX 和 TensorFlow 互操作性方面取得了长足进步，特别是 jax2tf 的开发。  
JAX的特点主要有：1. NumPy加速器，让模型很轻松在GPU和TPU上运行；2. XLA（Accelerated Linear Algebra）就是加速线性代数，一个优化编译器。JAX建立在XLA之上，大幅提高了JAX计算速度的上限；3. JIT。研究人员可使用XLA将自己的函数转换为实时编译（JIT）版本，用户可以用函数修饰符将将计算速度提高几个数量级。此外，JAX与Autograd完全兼容，支持自动差分，通过grad、hessian、jacfwd和jacrev等函数转换，支持反向模式和正向模式微分，并且两者可以任意顺序组成。以上也是JAX的优点。  
当然JAX也有一些不友好的地方：1.未针对CPU计算做充分优化；2.未形成像TensorFlow那样完整的基础生态以及产品；3. Debug困难；4. 不支持Windows系统；5. 不是一个深度学习框架，没有数据加载模块，需要嵌入TensorFlow或PyTorch中使用。  
- [OpenPPL-public/ppl.nn：RISC-V 最新进展介绍 | OpenPPL](https://mp.weixin.qq.com/s/LrYfOZ3czj1vmfmUaBLpBA)
摘要：今年一月，开源高性能深度学习推理引擎 OpenPPL 正式支持 RISC-V。随着夏天的到来，OpenPPL 继续在 RISC-V 上发力，并取得了较为显著的性能优势。  
本次 PPLCV 新增了对 RISC-V 平台的支持，具体如下：使用新版本 intrinsic，兼容性提升，支持动态向量长度；算子实现上，考虑到 cv 算子多通道格式、多数据类型等的灵活性，开发时尽量避免使用汇编，统一使用 intrinsic 来使用向量指令集 (RVV)，从而提高了代码的可读性以及缩短了开发周期；该版本由于编译器已支持 LMUL>1，在算子实现上通过合理配置 LMUL 值来使用了 Vector Register Grouping 功能，能够有效地提高指令执行效率，从而达到更高的性能。  
RVV 指令集中提供的 SEGMENT 系列指令非常适合处理 cv 中通道数 channels=3 的情况，但由于目前编译器会引入过多无关指令使得其性能不如其他指令，故是否使用该系列指令还需实际进行考量。  
- [paddlepaddle/paddle：飞桨框架v2.3发布，高复用性算子库、异构多云分布式训练等多项新特性升级 | 飞桨PaddlePaddle](https://mp.weixin.qq.com/s/X7SjN_TWJsOHUormhdbt1A)  
摘要：该版本特点：更加丰富的API体系、高复用性算子库PHI、高扩展性参数服务器、全流程硬件感知的性能自动调优、自动化压缩和推理引擎性能优化、大模型训练和推理能力全面升级、异构多云自适应分布式训练架构。  
考虑到高频的基础操作，打造了高复用性的算子库PHI，通过函数式接口为高阶开发者提供简单的算子开发方式，降低芯片对框架算子的适配成本。下面具体看一下PHI算子库的三个特点：  
1. 函数式算子接口，可以通过函数调用的方式复用基础算子来进行更复杂的组合算子开发。例如，通过对矩阵乘、加减乘除法等基础算子函数式接口的调用，很容易实现一个FC或者SGD算子，高效支撑自定义算子开发需求。以非常复杂的Einsum算子为例，通过利用PHI算子库，这类复杂算子的开发有低成本的显著优势；  
2. 插件式算子管理机制，避免和框架代码耦合，支持低成本地复用硬件加速库的能力；  
3. 提供Primitive的算子内核开发接口，实现不同芯片之间的算子内核开发代码的复用，提升算子内核的开发效率。例如，飞桨在GPU和XPU上，已实现了大量基于Primitive接口的算子内核的复用，大幅降低了硬件算子适配的成本。  


## 博文


- [Tenstorrent芯片架构浅谈 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/y-P1X-QeLjozC7jvEeAolA)  
摘要：近年市场上的AI芯片层出不穷，根源上还是算法与应用均处于高速迭代，计算硬件底座自然需要不断更新。其中芯片公司Tenstorrent的芯片架构别具一格，本文尝试一探究竟。  
Tenstorrent共设计出3款芯片，其中Jawbridge是一款小型测试芯片，Grayskull和Wormhole则是对外商用芯片，可覆盖训练和推理场景。Tenstorrent的芯片架构设计目标是解决模型在训练或推理时无法高效灵活扩展（scale out）的问题，提出2个核心技术点：
    1. 摒弃传统的核间共享式内存架构，采用Multicore Private Memory Model：芯片重点在硬件层面和软件层面分别加强了数据通信能力。在硬件层自研片上互联Network on Chip(NoC), NoC是2D双向环路结构；
    2. 动态执行：
        2.1 运行时数据压缩，以设计较小容量的private memory，在芯片内多核间及芯片间的数据通信量也随之降低，进而从整体上可以获得更高的性能功耗比。另外，Packet Manager还可以处理reshape/flatten等tensor形状变换操作，且该操作可以与Compute Engine并行执行，时间上可以overlap；
        2.2 条件执行：片上的逻辑控制单元和计算单元均可以高效运行，避免了CPU fallback的问题，基于稀疏门控专家混合模型，可通过门口网络实现只激活模型的部分结构，在增加模型容量和能力下不会成比例增加计算量；
        2.3 稀疏计算：支持常规的对权重进行稀疏化之外，还支持对激活值进行分块稀疏，进而降低计算量；
        2.4 动态混合精度：可以在运行时或AOT阶段设置每个算子的计算精度。  
Tenstorrent通过软硬协同设计方式，将数据并行和模型并行的部分功能实现下沉到硬件层，有效解决了横向扩展问题，这样就可以替代当前主流深度学习框架在分布式实现方面的大量编码工作，进而降低了深度学习框架的开发和使用门槛。同时，硬件的变化并没有降低软件栈的通用性，其软件栈支持PyTorch等主流框架。另一方面，芯片具有高度模块化，多个芯片可通过标准以太网端口连接在一起，进而扩展成大型AI网络。由于芯片内已集成NoC，因此这种扩展并不需要额外的交换机，因此扩展灵活度很高。  
- [MegEngine Inference 卷积优化之 Im2col 和 winograd 优化 | MegEngine Bot](https://zhuanlan.zhihu.com/p/532187602)  
摘要：卷积在推理时的优化有方式，本文主要介绍 Im2col+matmul 卷积以及 Winograd 卷积方式。  
im2col+matmul：和矩阵乘具有很多相似的特点，因此该方法使用 Im2col 的操作将卷积运算转化为矩阵运算，最后调用高性能的 Matmul 进行计算。该方法适应性强，支持各种卷积参数的优化，在通道数稍大的卷积中性能基本与 Matmul 持平，并可与其他优化方法形成互补。  
Winograd：按照 Winograd 算法的原理将卷积进行转变，达到减少卷积运算中乘法总量。其主要是通过将卷积中的乘法使用加法来替换，并把一部分替换出来的加法放到 weight 的提前处理中，从而达到加速卷积计算的目的。Winograd 算法的优化局限为在一些特定的常用卷积参数才支持。  
- [向外借力：Pluto助力MLIR编译器的多面体优化 | 壁仞科技研究院](https://mp.weixin.qq.com/s/n33DyOeTjA93HavZBZb94g)  
摘要：多面体编译是一项成熟的编译优化技术，演进了几十年，在传统的编译器中常作为一种优化工具使用，比如LLVM中使用的Polly，在GCC中使用的GRAPHITE。近些年来，多面体技术也引入到AI编译器中，进行循环优化及算子融合优化等。  
本文将关注在MLIR中以类插件的形式引入多面体优化技术，补充其多面体优化能力。多面体编译优化关注的是在确保程序执行正确的前提下重组多重循环的结构，实现性能的最优化。变形的目的是为了实现并行计算，达到更好的性能。  
多面体优化是一项成熟的技术，但也受限于对仿射变换的依赖，对无法进行仿射的循环的优化能力较弱，存在一定的局限性，因此无法在工业界得到广泛应用。同时，多面体优化技术理论相对复杂难懂，从事相关研究的人员较少，难以进行落地。尽管如此，多面体技术在解决特定的问题方面尤其独特的作用，比如在深度学习领域，对多算子融合和多层循环优化方面有着极大的帮助，可以将现有的多面体技术引入到AI编译器中，进行特定功能的优化。  
