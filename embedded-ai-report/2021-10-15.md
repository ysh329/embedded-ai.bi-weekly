---
layout: default
---

# 嵌入式AI简报 (2021-10-15)：【】【】【】【】【】【】


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：

好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [苹果A15芯片评测：CPU和GPU提升惊人 | anandtech 半导体行业观察](https://mp.weixin.qq.com/s/K7l3ci3So7Dk4xkXW_4jlg)  
摘要：今年苹果在 A15 的公关方面有点奇怪，官方避免将新芯片与自己的 A14 进行任何世代比较。与过往不同的是，Apple 今年更喜欢与友商环境的比较中描述新 SoC；虽然这在 Mac 方面并不罕见，但在今年 iPhone 发布会上，情况比往年更加突出。关于 A15 的几个具体事实是，Apple 正在为其 CPU 使用新设计、更快的神经引擎、新的 4 核或 5 核 GPU（取决于 iPhone 版本），以及全新的显示pipeline 和视频媒体硬件块编码和解码，以及新的 ISP 改进以提高相机质量。  
本文作为对新 SoC 的初始评测，重点关注在新芯片的性能和效率指标：
    1. 频率提升：3.24GHz 性能核和 2.0GHz的效率内核；
    2. 巨型缓存：性能 CPU的 L2 增加到 12MB，SLC 增加到 32MB。Apple 的 SLC 可能是芯片能效的关键因素，能够将内存访问保持在同一个硅片上，而不是使用速度更慢、功率效率更低的 DRAM。我们已经看到更多 SoC 供应商采用了这些类型的最后一级缓存，但在 32MB 的情况下，新的 A15 使竞争对手的实现相形见绌，例如骁龙 888 上的 3MB SLC或 Exynos 2100 上估计的 6-8MB SLC ；
    3. CPU 微架构变化：缓慢的一年？苹果今年更温和的微架构变化的可能是几个原因造成的——苹果在 2019 年失去了他们在大型性能核心以及部分设计团队的首席架构师（Nuvia）（后来在今年早些时候被高通收购）。向 Armv9 的转变也可能意味着在设计上做了更多的工作，而疫情也可能导致了一些非理想的执行。等到明年的 A16，我们才能真正确定 Apple 的设计节奏是否已经放缓，或者这是否只是一个节点，或者只是下一个微架构发生更大变化之前的平静。
    4. GPU 性能：出色的 GPU，一般般的散热设计。在这一代的A15 芯片上，GPU的配置很有趣，这是苹果第一次在 iPhone 设备范围内对其 SoC 上的 GPU 配置进行功能分割。在微架构变化方面，新的 A15 GPU 似乎采用了与 M1 GPU 相同的双倍 FP32 吞吐量，似乎在现有的 FP32/双倍速率 FP16 ALU 旁边增加了额外的单元。增加的 32MB SLC 也可能对 GPU 带宽和hit-rates有很大帮助，因此这两个变化似乎是大幅增加的最明显的解释。
总体而言，正如前面第1/2点我们说道，等到明年的 A16，我们才能真正确定 Apple 的设计节奏是否已经放缓，或者这是否只是一个节点，或者只是下一个微架构发生更大变化之前的平静。  


## 论文

- [2109.15099] [PPLCNet：CPU端强悍担当，吊打现有主流轻量型网络，百度提出CPU端的最强轻量型架构 | AIWalker](https://mp.weixin.qq.com/s/4QJaRIGFYzQG9UHstsECgQ)  
文章：https://arxiv.org/pdf/2109.15099.pdf  
代码: https://github.com/PaddlePaddle/PaddleClas
摘要：自从ResNet以来，无论是轻量型还是高性能网络均重度依赖跳过连接、残差连接这种机制。反而像MobileNetV1这种非常简单模型的性能提升鲜少有学者进行深入研究。这篇文章是看不到创新的，但是工程性的梳理太令人钦佩了。在这样一个“每天都有几十篇AI相关paper”的时代，能这样静下来去深挖这些被忽视的细节并精心整理。  
本文提出一种基于MKLDNN加速的轻量CPU模型PP-LCNet，它在多个任务上改善了轻量型模型的性能。本文列举了一些可以提升模型精度且保持延迟几乎不变的技术，基于这些改进，所提PP-LCNet可以凭借同等推理速度大幅超过其他已有网络。 在图像分类任务方面，所提PP-LCNet在推理延迟-精度均衡方面大幅优于ShuffleNetV2、MobileNetV2、MobileNetV3以及GhostNet；在其他下游任务(如目标检测、语义分割等)，所提方案同样表现优异：
    1. Better activation function：采用H-Swish替换BaseNet中的ReLU，性能大幅提升，而推理速度几乎不变；
    2. SE modules at appropriate positions：当把SE置于模型的尾部时，它具有更好作用 。因此，我们仅将SE模块添加到接近网络尾部的模块 ，这种处理方式具有更好的精度-速度平衡。注：SE模块采用了与MobileNetV3相似的机制：SE中的两个激活函数分别为SE和H-Sigmoid；
    3. Larger convolution kernels：卷积核的尺寸通常会影响模型最终的性能，MixNet的作者分析了不同尺寸卷积对于网络性能的影响并提出了混合不同尺寸的卷积核，然而这种操作会降低模型的推理速度。我们尝试仅使用一个尺寸的卷积，并在低延迟&高精度情形下使用大尺度卷积核。类似SE模块的位置，在网络的尾部采用卷积核可以取得全部替换相近的效果。因此，我们仅在网络的尾部采用卷积；
    4. Larger dimensional conv layer after GAP：在本文所提PP-LCNet中，GAP后的输出维度比较小，直接添加分类层会有相对低的性能。为提升模型的强拟合能能力，我们在GAP后添加了一个1280维的卷积，它仅需很小的推理延迟即可取得更强的性能。



## 开源项目



## 博文

