---
layout: default
---

# 嵌入式AI简报 (2021-10-15)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：

好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 三星：将会是世界上第一家量产 Gate All Around FET（GAA）架构的半导体厂，第一个 3GAE（3nm Gate-All-Around Early）技术预计 2022 年量产。半导体线宽微缩趋势遭遇许多技术上的困难导致进度放缓，眼前鳍式场效电晶体FinFET架构再度面临微缩限制，这也是积极开发 Gate All Around FET（GAA）架构，并且尽快在 3nm 工艺技术上导入多桥通道场效晶体管 Multi-Bridge-Channel FET（MBCFET）技术最关键的原因；三星将量产特斯拉的新型自动驾驶 5nm 芯片，早先于今年1月特斯拉已与三星合作研发；
- 紫光展锐：继推出T618、T610后，新一代 4G 平台T616和T606发布。T616是一款影像能力进一步提升，基于DynamIQ新一代大小核架构设计，2xA75@2.0Gz + 6xA75@1.8GHz + Mali G57 GPU。T606注重性能与能耗的均衡，同样基于DynamIQ新一代大小核架构设计，12nm 工艺的 2 x A75@1.6GHz + 6 x A55 ；
- 黑芝麻智能：完成数亿美元的战略轮及C轮两轮融资。战略轮由小米长江产业基金、富赛汽车等国内产业龙头企业参与投资。融资用于下一代高性能大算力自动驾驶平台的研发、人才引进、市场拓展和商业化。目前已基于两大核心自研IP：NeuralIQISP图像信号处理器及高性能深度神经网络算法平台DynamAI NN引擎，开发了多款自动驾驶芯片，以及自动驾驶计算平台；
- 毫末智行：孵化自长城汽车的自动驾驶公司，官宣推出智慧领航辅助驾驶方案主要针对的是高速、城市快速路等环路场景，即将上车摩卡、坦克300等长城旗下车型。其中一套感器硬件方案，简称5R1V。即1个摄像头和5个毫米波雷达，摄像头供应商Mobileye和毫米波雷达供应商博世。另一套更加追求安全冗余的高级版本方案，14颗摄像头+5个毫米波雷达+2个激光雷达。还有高精度地图配合，如高德，POC方案也在尝试与腾讯地图合作；
- Esperanto：RISC-V架构 AI 芯片公司，成立于 2014 年，已通过三轮融资筹集了 1.24 亿美元，于 2020 年 12 月发布了基于开放式 RISC-V 架构的 7nm ET-SoC-1 机器学习芯片。


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [苹果A15芯片评测：CPU 和 GPU 提升惊人 | anandtech 半导体行业观察](https://mp.weixin.qq.com/s/K7l3ci3So7Dk4xkXW_4jlg)  
摘要：今年苹果在 A15 的公关方面有点奇怪，官方避免将新芯片与 A14 等任何世代比较。与过往不同的是，Apple 今年更喜欢与友商环境的比较中描述新 SoC；虽然这在 Mac 方面并不罕见，但在今年 iPhone 发布会上。关于 A15 的几个具体事实是，**Apple 正在为其 CPU 使用新设计、更快的神经引擎、新的 4 核或 5 核 GPU（取决于 iPhone 版本），以及全新的显示pipeline 和视频媒体硬件块编码和解码，以及新的 ISP 改进以提高相机质量**。  
本文作为对新 SoC 的初始评测，**重点关注在新芯片的性能和效率指标**：
    1. **频率提升**：3.24GHz 性能核和 2.0GHz的效率内核；
    2. **巨型缓存**：性能 CPU的 L2 增加到 12MB，SLC 增加到 32MB。Apple 的 SLC 可能是芯片能效的关键因素，能够将内存访问保持在同一个硅片上，而不是使用速度更慢、功率效率更低的 DRAM。我们已经看到更多 SoC 供应商采用了这些类型的最后一级缓存，但在 32MB 的情况下，新的 A15 使竞争对手的实现相形见绌，例如骁龙 888 上的 3MB SLC或 Exynos 2100 上估计的 6-8MB SLC ；
    3. CPU **微架构变化**：缓慢迭代一年？更温和的微架构变化的可能是几个原因造成的，1. 首席架构师（Nuvia）离职；2. 向 Armv9 的转变可能意味更多工作；3. 疫情。等到明年的 A16，才能真正确定 Apple 的设计节奏是否已经放缓，或者这是否只是一个节点。
    4. GPU 性能：出色的 GPU，一般般的散热设计。在这一代的A15 芯片上，GPU的配置很有趣，这是**苹果第一次在 iPhone 设备范围内对其 SoC 上的 GPU 配置进行功能分割**。在微架构变化方面，新的 A15 GPU 似乎采用了与 M1 GPU 相同的双倍 FP32 吞吐量，似乎在现有的 FP32/双倍速率 FP16 ALU 旁边增加了额外的单元。增加的 32MB SLC 也可能对 GPU 带宽和hit-rates有很大帮助。  
- [壁仞科技首款高端通用 GPU 芯片交付流片 | 壁仞科技](https://mp.weixin.qq.com/s/nsiNyJZzsbCHza0Xo-zyMQ)  
摘要：首款通用GPU——BR100，于近日正式交付开始流片，预计明年市场发布。**GPU——BR100，性能参数直接对标当前国际最领先的同类产品**，具有高算力、高通用性、高能效三大优势，采用 7nm 制程工艺，完全依托壁仞科技自主原创的芯片架构。  
搭载 BR100 芯片的系列通用计算产品，**主要聚焦于人工智能训练和推理、通用运算等众多计算应用场景**，将弥补人工智能应用的高速发展带来的巨大算力缺口，可广泛应用于包括智慧城市、公有云、大数据分析、自动驾驶、医疗健康、生命科学、云游戏等领域。


## 论文


- [2109.15099] [PPLCNet：CPU 端强悍担当，吊打现有主流轻量型网络，百度提出 CPU 端的最强轻量型架构 | AIWalker](https://mp.weixin.qq.com/s/4QJaRIGFYzQG9UHstsECgQ)  
文章：https://arxiv.org/pdf/2109.15099.pdf  
代码: https://github.com/PaddlePaddle/PaddleClas  
摘要：自从 ResNet 以来，无论是轻量型还是高性能网络均重度依赖跳过连接、残差连接这种机制。反而像MobileNetV1这种非常简单模型的性能提升鲜少有学者进行深入研究。这篇文章是看不到创新，但工程性的梳理太令人钦佩了。  
**本文提出一种基于MKLDNN加速的轻量CPU模型PP-LCNet，多任务上改善了轻量型模型的性能**。**本文列举了一些可以提升模型精度且保持延迟几乎不变的技术，基于这些改进，所提PP-LCNet可以凭借同等推理速度大幅超过其他已有网络**。 在图像分类任务方面，所提PP-LCNet在推理延迟-精度均衡方面大幅优于ShuffleNetV2、MobileNetV2、MobileNetV3以及GhostNet；在其他下游任务(如目标检测、语义分割等)，所提方案同样表现优异：
    1. 激活函数：**采用H-Swish替换BaseNet中的ReLU，性能大幅提升，而推理速度几乎不变**；
    2. SE模块的位置：**将SE模块添加到接近网络尾部的模块 ，这种处理方式具有更好的精度-速度平衡**。注：**SE模块采用了与MobileNetV3相似的机制：SE中的两个激活函数分别为SE和H-Sigmoid**；
    3. 大卷积核：卷积核的尺寸通常会影响模型最终的性能，MixNet的作者分析了不同尺寸卷积对于网络性能的影响并提出了混合不同尺寸的卷积核，然而这种操作会降低模型的推理速度。我们**尝试仅使用一个尺寸的卷积，并在低延迟&高精度情形下使用大尺度卷积核**。类似SE模块的位置，**在网络的尾部采用卷积核可以取得全部替换相近的效果。因此，我们仅在网络的尾部采用了较大的 5x5 卷积**；
    4. 全局平均池化后的高维卷积：在本文所提PP-LCNet中，**GAP后的输出维度比较小，直接添加分类层会有相对低的性能。为提升模型的强拟合能能力，我们在GAP后添加了一个1280维的卷积，它仅需很小的推理延迟即可取得更强的性能**。  
- [1709.05943] [Fast YOLO：用于实时嵌入式目标检测 | 计算机视觉研究院](https://mp.weixin.qq.com/s/vaHqtdL6100brmfq0IBnyw)  
文章：https://arxiv.org/abs/1709.05943  
摘要：这篇文章翻译的不好，看看思想吧。  
提出的Fast YOLO框架分为两个主要部分：**第一，优化的YOLOv2 backbone架构**；**第二，运动自适应推理**，**并非所有捕获的视频帧都包含被检物体，因此不需要对所有帧进行深度推理。对于每个视频帧，由带有参考帧的视频帧组成的图像，传递到1×1卷积层，该卷积层的结果是一个运动概率图，再送入运动自适应推理模块以确定是否需要推理来计算更新的类概率图**。  
- [CPVR2021] [MobileHumanPose：装在手机里的3D姿态估计，模型尺寸仅同类1/7，平均关节位置误差却只有5厘米 | 量子位](https://mp.weixin.qq.com/s/wQcvdFlu8jn-P6-Qxwyv1w)  
文章：https://openaccess.thecvf.com/content/CVPR2021W/MAI/html/Choi_MobileHumanPose_Toward_Real-Time_3D_Human_Pose_Estimation_in_Mobile_Devices_CVPRW_2021_paper.html  
代码：https://github.com/SangbumChoi/MobileHumanPose
摘要：三维姿态估计需要平衡精度和计算量，本文提出的 MobileHumanPose 可以同时做到又块又好。其模型的尺寸只有ResNet-50的模型的1/7，算力达到了3.92GFLOPS。且平均每关节位置误差（MPJPE），也只有大约5厘米。  
该模型是一个从基本的编码器-解码器结构改良得来的模型。编码器用于全局特征提取，而解码器进行姿态估计的基础架构上，研究团队对其主干网络、激活函数，以及Skip concatenation功能都进行了修改：团队**选择的主干网络是 MobileNetV2 ，在其前四个倒置残差块（Residual Block）处修改了通道大小，激活函数采用 PReLU ，获得了性能提升**。但考虑到推理速度，团队使用Skip concatenation结构。这一结构能从编码器到解码器中导出低级别特征信号（Lowlevel feature signal），不会降低性能。  
- [ECBSR：移动端超分的磁悬浮，推理仅需10ms！港理工&达摩院开源超轻量超分网络ECB | 极市平台](https://mp.weixin.qq.com/s/wsA8XP4ej9lbFVRPB7E7CA)  
文章：https://www4.comp.polyu.edu.hk/~cslzhang/paper/MM21_ECBSR.pdf  
代码：https://github.com/xindongzhang/ECBSR  
摘要：本文是香港理工&达摩院张磊团队（https://www4.comp.polyu.edu.hk/~cslzhang/）在移动端超分 方面的工作，已被ACM-MM2021接收。本文将low-level领域知识与重参数（推理时，多个卷积融合）思想进行了巧妙结合，提出了一种新颖的模块：Edge-oriented Convolution Block(ECB) 。**在low-level领域，图像的梯度是非常重要的一个考虑**，如何有效提升生成图像的边缘锐利度一直是业界的关注点。**ECBSR就巧妙的就一阶梯度与二阶段梯度进行了结合**。
Base Model采用DESR架构，卷积仅采用3x3和1x1，ECB模块对那个五个分支依次为：conv3x3；conv1x3->conv3x3；conv1x1->Sobel-Dx；conv3x3->Sobel-Dy；conv1x1->Laplacian，**研究表明：边缘信息对于超分任务非常有用 。不同于SPSR的显式利用梯度信息，我们采用隐式方式将Sobel梯度集成到ECB模块的第三与第四分支，最后的laplacian滤波器提取了二阶梯度，特征更稳定对噪声更鲁棒，采用Expanding-and-Squeezeing Conv带来的更宽的特征可以显著提升表达能力**。**推理时，ECB模块里的6个卷积和梯度计算都可视为线性编号，可以完全融合为一个3x3卷积，以便于高效推理**。  
基于ECB构建了超轻量型且性能同样突出的ECBSR，在x4任务输出为1080p，硬件平台为骁龙865DSP上：
    1. 当性能媲美SRCNN/ESPCN时，ECBSR-M4C8在移动端推理仅需10ms ，而SRCNN与ESPCN分别需要1583ms、26ms；
    2. 当性能媲美LapSRN时，ECBSR-M10C32在移动端推理仅需17ms ，而LapSRN则需要5378ms；
    3. 当性能媲美IMDN、EDSR以及CARN时，ECBSR-M16C64在移动端的推理仅需71ms ，而IMDN、EDSR与CARN的推理则分别为2782ms、527ms、170ms。


## 开源项目


- [华为昇腾：2.69分钟完成BERT训练！新发CANN 5.0加持，还公开了背后技术 | 量子位](https://mp.weixin.qq.com/s/KsSosQsb-KBvIpKsMEDqUA)  
摘要：昇腾选择用图计算的原理，分析集群训练的流水线分布、内存分配，针对不同机器的特点进行了架构上的设计，合理分配各个节点中的内存和通讯时间，来提高机器整体的计算效率。**CANN 5.0版本在性能优化上，主要自研了4点技术**：
    1. **任务自动流水**：数据加载是因为硬件需要一定的时间来“反应”，包括加载计算指令等，但在数据量大的情况下，这显然会极大地延缓整体计算时间。5.0实现了计算指令和数据载入的多流水并行，**载入数据满足分段数据量时，不仅启动后续计算逻辑、还保持数据继续载入**，进一步“压榨”硬件处理器的并行计算能力，实现任务衔接；
    2. **算子深度融合**：算子是支持AI模型训练与推理的基本运算单元及组合，异构计算架构基本都要有自己的算子库。5.0版本重新定制了**更灵活的算子融合规则，通过多个算子自动融合提升模型训练效率**；
    3. **自适应梯度切分**：该技术是华为**针对集群训练提出的智能梯度切分算法**，具体针对模型训练中的迭代计算进行了优化。CANN 5.0能通过**智能梯度切分算法，自动搜索出最优梯度参数切分方式，让计算和通信进一步并行执行，使得通信拖尾时间降至最低、梯度调优时间降低90%**；
    4. **AutoTune智能计算调优**：不同的AI模型，如果架构只用一种方式进行计算分配的话，势必会造成不适配的情况。因此，CANN 5.0研究出了**智能数据切分技术，提出最优切分策略，确保每个计算单元被充分利用，平均性能提升30%以上**。5.0版本也预置了海量模型优化，能极大地缩短开发者的调优时间。  
正是这些技术优势，让华为在AI性能提升上，拥有了更多的底气。此外，相比于昇腾CANN 3.0，“跨代”的5.0版本带来三大优势：1. 性能：AI模型训练/推理性能大幅提升，用时更短；2. 功能：推理引擎ATC Suite1.0首次发布，AI模型推理性能更高、功能更全面；3. **便捷性：代码开发和调试进一步简化，包括支持混合编程等**，使用门槛更低。  
- [OpenCV 4.5.4 发布：DNN对RISC-V的优化，8位量化功能和导入ONNX量化模型功能，Intel推理引擎OpenVINO后端增强，支持给更多模型人体分割等 | OpenCV团队](https://mp.weixin.qq.com/s/PEOxwkxEWanxlDu4PS9kNg)  
摘要：GSoC 2021结束了，11个项目的成果目前已经合入OpenCV 4.5.4（main repo和opencv_contrib）：**DNN模块8位量化功能和导入ONNX量化模型功能**、Julia语言绑定改进了一些、给了个语音识别示例、**OpenCV DNN对RISC-V的优化（中科院软件所贡献）**、Universial Intrinsics和parallel_for_使用教程；  
DNN模块：改进layers和activations，支持更多模型、GRU, CumSum, Max, Min, ExpandDims、修复卷积的非对称填充、修复Unsqueeze (ONNX opset 13)、修复OpenCL核的几个内存填充问题、实现TextRecognitionModel中的CTC前缀束搜索解码、增加SoftNMS；  
**Intel推理引擎后端（OpenVINO ）：增加OpenVINO 2021.4.1 LTS release的支持**、增加对非FP32输出或1D输出模型的支持；
更多详细信息请参考：https://github.com/opencv/opencv/wiki/ChangeLog#version454 。  


## 博文


- [深度学习编译系列之算子编译IR转换 | 商汤学术](https://mp.weixin.qq.com/s/oKBXuKGwhS9nYIUQa6yK0g)  
摘要：这篇小文给出了对于深度学习编译器在**一种长尾算子代码生成场景中IR stack的讨论**。**虽然单个长尾算子在整个神经网络中可能耗时比例较小，但是神经网络中可能包含数十个甚至数百个长尾算子，并且在未来这种需求也在不断地被创造**。   
因此，基于编译手段，不断增强支持不同类型的算子翻译，具有重要意义。当然，计算型算子和访存型算子有着不同的特征，计算型算子需要注意计算 pattern 的翻译，并需要满足形状信息约束，而访存型算子则需要注意 访存 pattern 的翻译，有些操作潜在囊括于 python 复杂的语法定义中，这时便需要在深度学习编译器中对应设计 IR 以支持 IR lowering 以及代码生成流程。  
通过上述讨论可以发现，**对于访存型算子的翻译和支持往往是长尾算子高性能代码生成的痛点和难点**。并且，DSC 往往对张量运算有很好的表示和支持方式， 本文的讨论也集中在张量运算上，而当我们面对越来越复杂的网络，Domain Specific Compiler 对于非张量运算支持的需求是否也在不断增加呢，例如控制流？我们在哪个层面对其进行表示和优化是更好的解决方案呢，希望和大家有更多的探讨。  
- [深度学习模型大小与模型推理速度的探讨 | OpenPPL](https://mp.weixin.qq.com/s/7ODywY3pP1sUtOBTxUW6sg)  
摘要：本文介绍了评估模型大小的四个常用指标——**计算量、参数量、访存量、内存占用**，从 RoofLine 模型入手详细讨论了**影响模型推理速度的影响因素**，并给出了**面向推理速度的模型设计方法论与建议**。撰写本文的目的，不仅仅是给算法同学提供有效的网络设计建议，更多的还是希望能够传达性能优化的基础知识与分析思路，减少算法设计到部署之间的 gap，更快速高效的设计推理友好的网络模型。  
- [C/C++ 性能优化背后的方法论：TMAM | vivo互联网技术](
摘要：
- [金山办公 TFLite 技术分享之 《基于TFLite 的办公应用开发套件》 | bilibili](https://www.bilibili.com/video/BV1hQ4y1z7PW)  
摘要：TFLite SIG 邀请到 WPS 的齐南为大家做分享，分享主题是 《基于TFLite 的办公应用开发套件》。
