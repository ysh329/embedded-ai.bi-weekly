---
layout: default
---

# 嵌入式AI简报 (2021-08-21)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：


- 台积电：投行发布的消息证实，台积电（TSMC）先进的3nm芯片制造工艺已步入明年量产的轨道；
- 谷歌：谷歌芯实为三星 Exynos9855，“Tensor SoC”的原创度远没有外界想象的那么高，因为它的产品代号除了目前已知的“Whitechapel”和“GS101”之外，还有一个就是“Exynos 9855”，实由三星研发生产，产品定位高于Exynos2100（型号名S5E9840），低于Exynos2200（型号名S5E9925）；
- 英伟达：2021Q2创下 65.1 亿美元的纪录，较2021年同期增长 68%，较Q1增长 15%，**公司的游戏、数据中心部门推动了净利润同比增长282%，以及专业视觉平台均创下收入纪录**；
- 台积电：**市值超越腾讯，成亚洲市值最高的公司**。截至8月18日上午亚洲时段的数据，台积电目前在亚洲公司中以超过 5380 亿美元的市值位居榜首。腾讯和阿里巴巴分别以超过 5360 亿美元和 4720 亿美元分别占据市值第二和第三的位置；
- 联发科：天玑920/810 5G芯片，接替现任天玑900和天玑800。**920采用6nm制程打造，2xA78@2.5GHz +6xA55@2.0GHz + Mali-G68**，天玑810，同样采用6nm打造，八核心CPU设计，包含主频为2.4GHz的Cortex-A76核心，GPU集成G57 MC2，虽然CPU频率比天玑800高，但GPU反而有所削弱，支持120Hz刷新率全高清FHD+显示；
- 比亚迪半导体：**上市被“中止”，原因是被律所“连坐”。比亚迪半导体用不到短短两个月迅速完成两轮融资。其中，5月26日宣布完成的A轮融资（19亿元）和6月15日宣布完成的A+轮融资（8亿元），中间仅相距20天。估值超100亿**。根据招股书，其拥有功率半导体（最核心，近三年来营收占比徘徊在30%）、智能控制IC、智能传感器、光电半导体等产品的研发设计能力以及全套产线；
- 高通：① 推出全球首个由5G和AI赋能的无人机平台和参考设计——**Qualcomm Flight RB5 5G平台，能够以超低功耗支持高性能异构计算，提供高能效边缘侧推理**；② 由高通创投投资的格科微有限公司（简称：“格科微”，股票代码：688728）登陆科创板。创立于2003年，业务为CMOS图像传感器和显示驱动芯片的研发、设计和销售。**未来，格科微还将通过自建部分12英寸BSI晶圆后道制造产线、12英寸晶圆制造中试线、部分OCF制造及背磨切割产线的方式，巩固产能保障力度，提升在高阶产品领域的研发能力和研发速度，并加强对供应链产能波动风险的抵御能力**；③ **高通宣布计划以46亿美元（约合297.2亿元人民币）的价格收购瑞典自动驾驶控制、传感器零部件供应商维宁尔**，而高通去年发布Snapdragon Ride自动驾驶解决方案，或将**加速自动驾驶解决方案研发进展**；
- 商汤：**传言将启动IPO，计划未来几周在港交所提交IPO申请**。若属实，将成为继旷视科技、云知声、依图科技后，又一家上市的人工智能公司；
- 燧原：**投资成立半导体公司，入局集成电路芯片设计和服务**。此前燧原先后发布两代AI训练芯片，并与腾讯基于业务真实场景开展了深入合作，其执行力和落地能力已得到了证明；
- 爱芯科技：完成**数亿元A+轮融资总金额达数亿元人民币**，韦豪创芯、美团联合领投。成立于2019年5月，**专注于研发高性能、低功耗的人工智能视觉处理芯片，并自主开发面向推理加速的神经网络处理器**。其自主研发的第一颗AI芯片——AX630A已达成量产状态，这一针对边缘侧、端侧应用的人工智能视觉芯片，在算法与硬件的深度结合下，可提供高品质的视频图像质量，支持物体检测、人脸识别等多种AI视觉任务，应用前景十分广阔。继AX630A进入量产后，爱芯科技自主研发的第二颗芯片日前也已回片并成功点亮。未来，爱芯科技将在保障AX630A现货供应的基础上，持续研发更多具有差异化的产品，适应端侧与边缘侧复杂的应用环境，赋能各行各业实现智慧化发展，真正实现“AI改变生活”；
- 三星：Exynos 2200将配备6核心的**RDNA2架构GPU，其GPU测试能领先骁龙888达40%，但由于原材料供应、产能、工艺等方面的限制，可能要在大批产品上放弃搭载该芯片的想法**。下一代旗舰级平板产品Galaxy Tab S8/S8+/S8 Ultra可能将转而搭载高通下一代平台骁龙898芯片；三星将使用新思科技（Synopsys）提供的AI功能——DSO.ai——来设计下一代Exynos处理器，新思科技表示 DSO.ai 是第一个用于处理器设计的商业AI软件，其对设计芯片的工程周期从6个月缩短到一个月，芯片性能也有提升；
- AMD：Mercury Research 的最新数据显示，今年 2 季度，AMD 已成功占据 x86 处理器市场的 22.5% 份额。这是自 2007 年以来的最高纪录、并且接近 2006 年 25.3% 的历史峰值。不过英特尔的份额依然高达 77.5%，而威盛（VIA）等少数 x86 处理器厂家的全球份额几乎可以忽略不计。这份报告还提供了更细致的市场划分统计，可知 AMD 台式 CPU 的份额略有下滑，从 2021 年 1 季度的 19.3%、降到了 2 季度的略高于 17% ；
- Intel：**发布基于 Xe HPG 微架构的高性能显卡 Arc** ，并融合 Xe LP、HP 和 HPC 微架构的优势，**专为消费端打造**，涵盖硬件、软件和服务三方面。代号为 Alchemist 的第一代 Arc 产品将采用基于硬件的光线追踪和A.I.驱动的超级采样，为 DirectX 12 Ultimate 提供全面支持，并**将于 2022Q1 上市**；
- 轻舟智航：继2021年年初完成A轮融资之后，**无人驾驶头部公司轻舟智航又获得了1亿美元A+轮融资**。该公司2019年3月成立于硅谷，当时自动驾驶RoboTaxi、货运、低速物流车等等都已经形成明显头部阵营。**该笔融资的落地能够加速轻舟自动驾驶“超级工厂”的落地。为实现自动驾驶的商业化及规模化**；
- pony.ai: 传小马智行暂缓赴美上市。公司回应称：公司并未确认过上市计划抑或上市时间线，已经暂缓了通过SPAC以120亿美元估值赴美上市的计划。知情人士称，这家由丰田汽车支持的初创公司将寻求通过私募融资，估值为120亿美元；
- 壁仞科技：**前AMD全球副总裁李新荣（Allen Lee）先生加入壁仞科技**，出任联席CEO，专注组织，管理及产品设计端。李新荣先生在GPU领域拥有超过30年的丰富经验，在AMD就职15年中，一手构建规模数千人的研发团队，并实现了团队研发能力从单项到覆盖“端到端”完整项目流程的大突破；

> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [来自莱斯大学的初创公司ThirdAI：致力于AI的通用CPU算法，干掉GPU | 半导体行业观察](https://mp.weixin.qq.com/s/BayDiRU26soehbdw_1c1CA)  
摘要：致力于降低 AI 深度学习成本的初创公司 ThirdAI ，利用其算法和软件创新，使通用中央处理器 (CPU) 比用于训练大型神经网络的图形处理单元更快。他们推出了 SLIDE（Sub-Linear Deep Learning Engine：次线性深度学习引擎），这是一种部署在通用 CPU 上的算法，生成可以对抗 GPU 的主导地位。  
**SLIDE 提供的结果比可用的最佳 Tensorflow GPU 硬件快 3.5 倍，性能比 Tensorflow CPU 高 10 倍**。 **SLIDE 使用采样哈希表，特别是修改后的局部敏感哈希（LSH），来快速查询神经元 ID 以进行激活，而不是逐个矩阵计算整个网络矩阵**。  
**通过使用多核 CPU 处理和优化——以及局部敏感散列 (LSH) 和自适应丢失——SLIDE 实现了，无论批量大小如何，都可以做到 O(1) 或恒定时间的复杂度**，正是得益于这样的设计，该公司获得了Neotribe Ventures、Cervin Ventures 和 Firebolt Ventures的投资，该投资将用于雇用更多员工并投资于计算资源。  
- [谷歌安卓上新功能：用脸就能控制手机 | 智东西](https://mp.weixin.qq.com/s/hWQoa5uh7YSpoYxQhfP4gA)  
摘要：谷歌最近更新了其安卓无障碍（辅助功能）套件（Android Accessibility Suite），其中的“开关/切换访问（Switch Access）”功能列表增加了“摄像头开关（Camera Switch）”功能。开关访问功能用于设置用户操控手机的快捷方式，比如通过蓝牙、USB连接，用户可以设置无线耳机、外接键盘等设备如何操控手机，现在谷歌又增加一种“非接触式”操控方式——用脸。  
有了这一功能，用户可以用张嘴、微笑、扬眉、向左看、向右看和向上看这6种表情来操控手机，执行“选择”、“下一步”、“返回”等十几种操作。  
- [UC伯克利博士尤洋回国创业，求学期间破ImageNet纪录！已获超千万种子轮融资 | 量子位](https://mp.weixin.qq.com/s/gKGdog38zjx4HUU-eYzqVQ)  
摘要：尤洋，是LAMB优化器的提出者，曾成功将预训练一遍BERT的时间，从原本的三天三夜一举缩短到一个多小时。英伟达官方GitHub显示，LAMB 比 Adam 优化器可以快出整整 72 倍。　　
尤洋已在UC伯克利获得了博士学位，目前**回国并创立潞晨科技。公司主营业务包括分布式软件系统、大规模人工智能平台以及企业级云计算解决方案**。地点在北京中关村，目前已经获得由创新工场和真格基金合投的超千万元种子轮融资。
单纯地堆硬件，并不能解决所有问题。一方面，当硬件数量达到一定量后，堆机器无法带来效率上的提升；另一方面，中小企业往往没有足够的资金支持如此大规模的硬件部署。因此，优化技术成为了绝佳选择。**潞晨科技就是旨在打造一个高效率低耗能的分布式人工智能系统。它可以帮助企业在最大化提升人工智能部署效率的同时，还能将部署成本最小化**。而且潞晨打造的系统是一个通用系统，对大部分超大模型都有效。就目前的Transformer应用而言，该系统在同样的硬件上相对业界最好的系统，可以提升2.32倍的效率。以上他们打造的通用系统，依旧离不开LAMB（Layer-wise Adaptive Moments optimizer for Batch training）方法。　　
尤洋他们在研究中发现，在不改变硬件设置的情况下，**能耗主要来自于数据移动。数据移动包括集群内服务器之间的通讯、GPU与CPU之间的通讯、CPU与磁盘的通讯等等。为此，他们还实现了一套基于通讯避免算法的系统**。可以在不增加计算量的情况下有效减少数据移动量，从而减少能耗。　　
- [OpenAI CLIP模型袖珍版，24MB实现文本图像匹配，iPhone上可运行 | 机器之心](https://mp.weixin.qq.com/s/xfHTaHS_fl1556A5aNSDcQ)  
摘要：最近，PicCollage 公司在自己的内容产品上对 CLIP 模型进行了测试，并获得满意的性能。350MB 的FP32 原始模型（可称为 teacher 模型）蒸馏后降为 48MB（student 模型）。**在单个 P100 GPU 上训练了数周后，将 48MB 大小的 FP32 student 模型转换成了 CoreML 格式的 FP16 24MB 大小的模型，精度为 FP16，性能变化几乎可以忽略不计，蒸馏后的模型可以在 iPhone 等 IOS 设备上运行**。  


## 论文


- [Only Train Once：微软、浙大等研究者提出剪枝框架OTO，无需微调即可获得轻量级架构 | 机器之心](https://mp.weixin.qq.com/s/_uUtDFL7lhxTkVR_Sl-VPQ)  
链接：https://arxiv.org/pdf/2107.07467.pdf  
摘要：来自微软、浙江大学等机构的研究者提出了一种 one-shot DNN 剪枝框架，无需微调即可从大型神经网络中得到轻量级架构，在保持模型高性能的同时还能显著降低所需算力。该研究的主要贡献概括如下：  
    1. One-Shot 训练和剪枝。研究者提出了一个名为 OTO（Only-Train-Once）的 one-shot 训练和剪枝框架。它可以将一个完整的神经网络压缩为轻量级网络，同时保持较高的性能。OTO 大大简化了现有剪枝方法复杂的多阶段训练 pipeline，适合各种架构和应用，因此具有通用性和有效性。  
    2. Zero-Invariant Group（ZIG）。研究者定义了神经网络的 zero-invariant group。如果一个框架被划分为 ZIG，它就允许我们修剪 zero group，同时不影响输出，这么做的结果是 one-shot 剪枝。这种特性适用于全连接层、残差块、多头注意力等多种流行结构。  
    3. 新的结构化稀疏优化算法。研究者提出了 Half-Space Stochastic Projected Gradient（HSPG），这是一种解决引起正则化问题的结构化稀疏的方法。研究团队在实践中展示并分析了 HSPG 在促进 zero group 方面表现出的优势（相对于标准近端方法）。ZIG 和 HSPG 的设计是网络无关的，因此 OTO 对于很多应用来说都是通用的。  
实验结果。利用本文中提出的方法，研究者可以从头、同时训练和压缩完整模型，无需为了提高推理速度和减少参数而进行微调。在 VGG for CIFAR10、ResNet50 for CIFAR10/ImageNet 和 Bert for SQuAD 等基准上，该方法都实现了 SOTA 结果。
- [MicroNets：更小更快更好的MicroNet，三大CV任务都秒杀MobileNetV3 | 我爱计算机视觉](https://mp.weixin.qq.com/s/8Veob1WRd3S-CnOdeedfRQ)  
摘要：本文旨在以极低的计算成本解决性能大幅下降的问题。**作者发现有两个因素是可以有效提高精度的，分别是：稀疏连通性和动态激活函数**。前者避免了网络宽度的大幅度缩减的危害，而后者则减轻了网络深度缩减的危害。这其中，作者首先证明了在给定的计算预算的情况下，**降低节点连通性以扩大网络宽度**为提供了一个很好的权衡。其次，依赖于**改进的层非线性来弥补减少的网络深度**，这决定了整个网络的非线性 。这两个因素激发了更有效卷积和激活函数的设计。  
技术上来讲，作者提出的微分解卷积（micro-factorized convolution），它将卷积矩阵分解成低秩矩阵，以将稀疏连通性整合到卷积中，即Micro-Factorized convolution，其**通过在pointwise和depthwise卷积上的低秩近似来实现通道数和输入/输出连接之间的平衡**。  
而新的动态激活函数，称为Dynamic Shift Max，**通过最大化输入特征图和圆形通道位移之间的多次动态融合来提高非线性，动态地融合了连续的通道组，增强节点连接性和非线性，以弥补深度的减少**。  
基于这两个操作，作者提出一个新的网络系列：MicroNet，在低FLOP状态下，它取得了显著的性能提高。MicroNet对三个CV任务（图像分类、目标检测和人体姿态估计）都实现了明显的改进。例如，在12M FLOPs的约束下，MicroNet在ImageNet分类上达到了59.4%的Top-1准确率，比MobileNetV3高出9.6%。  
- [1912.12795] [联邦迁移学习最新进展，计算和传输如何“限制”模型性能 | AI科技评论](https://mp.weixin.qq.com/s/WOeWOO0Jj2RNx1kk-gKFRw)  
标题：Quantifying the Performance of Federated Transfer Learning  
链接：https://arxiv.org/abs/1912.12795  
摘要：2018年，联邦迁移学习理论被提出。该理论中，训练所使用的多个数据集，无需保证特征空间的一致。另外，该理论使用同态加密替代差分隐私对隐私数据进行保护。这些改进为联邦学习在金融、医疗等场景中的应用带来了极大的便利。但是联邦迁移学习在实际使用中暴露出了严重的性能缺陷。  
针对这个问题，来自于香港科技大学、星云Clustar以及鹏城实验室的研究人员联合发表了《量化评估联邦迁移学习(Quantifying the Performance of Federated Transfer Learning)》。**该论文通过对联邦迁移学习框架进行研究，提出了联邦学习在实际应用中所面临的性能方面的挑战，并给出了相应优化方案**。  
- [二值化网络（BNN）究竟如何训练?这篇 ICML 2021 论文给你答案 | AI科技评论](https://mp.weixin.qq.com/s/0lQrK9uoCnntUXVxjLreiQ)  
论文：https://arxiv.org/abs/2106.11309  
代码：https://github.com/liuzechun/AdamBNN  
摘要：这篇最新来自CMU和HKUST科研团队的ICML 论文，仅通过调整训练算法就在ImageNet数据集上取得了比之前state-of-the-art 的BNN 网络 ReActNet 高1.1% 的分类精度，最终的top-1  accuracy达70.5%，超过了所有同等量级的二值化网络。  
**Adam优化的二值化网络中激活值过饱和问题和梯度消失问题都有所缓解。这也是Adam在BNN上效果优于SGD的原因，这篇论文通过一个构造的超简二维二值网络分析来分析Adam和SGD 优化过程中的轨迹，此外本文展示了一个很有趣的现象，在优化好的BNN中，网络内部存储的用于帮助优化的实数值参数呈现一个有规律的分布**。  
这篇论文发现，二值化网络中使用weight decay会带来一个困境：高weight decay会降低实值参数的大小，进而导致二值参数易变符号且不稳定。而低weight decay或者不加weight decay会使得二值参数将趋向于保持当前状态，而导致网络容易依赖初始值。**为了量化稳定性和初始值依赖性，该论文引入了两个指标：用于衡量优化稳定性的参数翻转比率（FF-ratio），以及用于衡量对初始化的依赖性的初始值相关度 (C2I-ratio)**。  
那么weight decay带来的稳定性和初始值依赖性的两难困境有没有方法解离呢?该论文发现最近在ReActNet (Liu et al., 2020) 和Real-to-Binary Network (Brais Martinez, 2020) 中提出的**两阶段训练法配合合适的weight-decay策略能很好地化解这个困境**。  
这个策略是，第一阶段训练中，只对激活值进行二值化，不二值化参数。由于实值网络不必担心二值化网络中的参数跳变带来的不稳定，可以添加weight decay来减小初始值依赖。随后在第二阶段训练中，二值化激活值和参数，同时用来自第一步训练好的参数初始化二值网络中的实值参数，不施加weight decay。**这样可以提高稳定性并利用预训练的良好初始化减小初始值依赖带来的弊端**。该论文综合所有分析得出的训练策略，在用相同的网络结构的情况下，取得了比state-of-the-art ReActNet 超出1.1%的结果。  


## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [mindspore-ai/mindspre: 端侧AI框架-MindSpore1.3 Lite的大更新 | MindSpore](https://mp.weixin.qq.com/s/zycfyM4XslTJptBzAN9amQ)  
摘要：上半年MindSpore陆续发布了1.2和1.3两个版本，端侧Lite这一块其实增加了很多大特性，前面没有太多宣传，这次统一把MindSpore Lite最新版本的主要新增能力给大家介绍一下。  
这个版本中全新的南向自定义能力，Delegate机制，针对微处理器的Micro方案，新增支持NNIE/NVIDIA GPU等硬件推理，完善控制流，支持流式执行，支持稀疏编码，端云联邦学习、性能更高的算子库等，给端侧AI带来更多的全新体验。下面就带大家快速浏览这两个版本的关键特性。  
- [dog-qiuqiu/Yolo-FastestV2：YOLO-FastestV2更快更轻！参数量仅250k，移动端高达300 FPS！ | 知乎](https://zhuanlan.zhihu.com/p/400474142)  
链接：https://zhuanlan.zhihu.com/p/400474142  
摘要：Yolo-Fastest注重的就是单核的实时推理性能，在满足实时的条件下的低CPU占用，不单单只是能在手机移动端达到实时，还要在RK3399，树莓派4以及多种Cortex-A53低成本低功耗设备上满足一定实时性，毕竟这些嵌入式的设备相比与移动端手机要弱很多，但是使用更加广泛，成本更加低廉。  
这一版的改进吧，首先模型的backbone替换为了shufflenetV2，相比原先的backbone，访存减少了一些，更加轻量，其次Anchor的匹配机制，参考的YOLOV5，其实YOLOV5与Darknet的官版YOLOV4在Anchor的匹配机制的区别还是挺大的，这点不细讲了，网上解析一大堆，其次是检测头的解耦合，这个也是参考YOLOX的，将检测框的回归，前景背景的分类以及检测类别的分类有yolo的一个特征图解耦成3个不同的特征图，其中前景背景的分类以及检测类别的分类采用同一网络分支参数共享。最后将检测类别分类的loss由sigmoid替换为softmax。  


## 博文


- [网易云信神经网络音频降噪算法：提升瞬态噪声抑制效果，适合移动端设备 | 机器之心](https://mp.weixin.qq.com/s/q6F61XHm9AJvbEounP_ZKw)  
摘要：网易云信音频实验室自主研发了一个针对瞬态噪声的轻量级网络音频降噪算法（网易云信 AI 音频降噪），对于 Non-stationary Noise 和 Transient Noise 都有很好的降噪量，并且控制了语音信号的损伤程度，保证了语音的质量和理解度。网易云信的 AI 降噪在 10ms 的音频帧数据（16kHz 采样率）中只需要约 400,000 次浮点计算，经过云信自研的 AI 推理框架 NENN 加速，在 iPhone12 上每 10ms 的运算平均时间低于 0.01ms，峰值时间低于 0.02ms，CPU 占比小于 0.02%。  
- [基于Tengine，打造 NVDLA 更友好的开源加速器工具链 | LeiWang1999 知乎](https://zhuanlan.zhihu.com/p/401943271)  
摘要：NVDLA的官方发布的工具链很弱，只能端到端地运行极为简单的分类网络，而现在在绝大部分的深度神经网络应用里分类往往只是其中一小部分。例如我现在想利用加速器去运行yolo，但其中有许多加速器并不支持的算子，加速器支持的convolution、pooling、relu等等算子最好都要用加速器运行，而那些不支持的算子则需要Fallback到CPU去运行。
本文要介绍的是笔者**利用Open AI Lab开源的边缘设备推理框架Tengine，为NVDLA打造一套新的工具链，并部署到自己的 FPGA 芯片上**。  
- [Y-tech 服务端特效平台设计与思考 | 快手Ytech](https://mp.weixin.qq.com/s/akfpyVHV30bNHQKRA1JIGQ)  
摘要：快手作为国内最主要的短视频社区之一，每天都产生海量的视频创作，其中“特效”视频是不可忽视的一部分。Y-tech 通过在计算机视觉，图形学以及机器学习的交叉领域不断探索，持续为快手提供令人惊艳的“特效”视频。  
支撑这些丰富的视频特效，不仅需要精巧的算法，同时还有图形和图像相关的推理和渲染带来的庞大计算量。由于部分特效消耗计算资源高，而端上计算资源有限；或是适配多种端上版本复杂性高等困难，诞生了服务端特效需求。通过用户上传媒体文件，服务端处理，下发给用户的流程，使更多复杂或带有新特性的魔表短期落地成为可能。  
- [计算图替代——一种DNN框架计算图优化方法 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/vcVJ3bYLoCv2UTgHzBjLuw)  
标题：Optimizing DNN Computation Graph using Graph Substitutions  
链接：http://www.vldb.org/pvldb/vol13/p2734-fang.pdf  
摘要：在DNN中每一回合的推理或训练中的每一次迭代通常可以表示为计算图，通过计算图优化可以提高DNN训练和推理的速度。目前主流的框架Tensorflow供了图优化器的API、TVM采用Op Fusion在内的多种计算图优化手段进行加速计算。  
**本文将主要介绍computation graph substitution优化方法**。计算图**替代就是找到另外一个计算图在功能上等效替代当前的计算图，在替代的同时可以减小计算时间以及计算量**。  
从现有的论文来看，**计算图替代可以起到一定的优化计算效果，需要将图形级和算子级优化结合起来**。这种联合优化具有挑战性，因为这两个问题都涉及到庞大且复杂的搜索空间，而一个级别的优化会影响另一个级别的搜索空间。未来研究方向应该是在减小搜索空间的同时进行最大限度的图替代，并且将计算图替代优化与其他优化方法结合，这样会给DNN框架优化计算带来最大的收益。  
