---
layout: default
---

# 嵌入式AI简报 (2021-08-21)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：

好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 爱芯科技：完成数亿元A+轮融资总金额达数亿元人民币，韦豪创芯、美团联合领投。成立于2019年5月，专注于研发高性能、低功耗的人工智能视觉处理芯片，并自主开发面向推理加速的神经网络处理器。其自主研发的第一颗AI芯片——AX630A已达成量产状态，这一针对边缘侧、端侧应用的人工智能视觉芯片，在算法与硬件的深度结合下，可提供高品质的视频图像质量，支持物体检测、人脸识别等多种AI视觉任务，应用前景十分广阔。继AX630A进入量产后，爱芯科技自主研发的第二颗芯片日前也已回片并成功点亮。未来，爱芯科技将在保障AX630A现货供应的基础上，持续研发更多具有差异化的产品，适应端侧与边缘侧复杂的应用环境，赋能各行各业实现智慧化发展，真正实现“AI改变生活”；
- 三星：
- AMD：Mercury Research 的最新数据显示，今年 2 季度，AMD 已成功占据 x86 处理器市场的 22.5% 份额。这是自 2007 年以来的最高纪录、并且接近 2006 年 25.3% 的历史峰值。不过英特尔的份额依然高达 77.5%，而威盛（VIA）等少数 x86 处理器厂家的全球份额几乎可以忽略不计。这份报告还提供了更细致的市场划分统计，可知 AMD 台式 CPU 的份额略有下滑，从 2021 年 1 季度的 19.3%、降到了 2 季度的略高于 17% ；
- Intel：发布基于 Xe HPG 微架构的高性能显卡 Arc ，并融合 Xe LP、HP 和 HPC 微架构的优势，专为消费端打造，涵盖硬件、软件和服务三方面。代号为 Alchemist 的第一代 Arc 产品将采用基于硬件的光线追踪和A.I.驱动的超级采样，为 DirectX 12 Ultimate 提供全面支持，并将于 2022Q1 上市；
- 轻舟智航：2019年3月成立于硅谷，当时自动驾驶RoboTaxi、货运、低速物流车等等都已经形成明显头部阵营；
- pony.ai: 传小马智行暂缓赴美上市。公司回应称：公司并未确认过上市计划抑或上市时间线，已经暂缓了通过SPAC以120亿美元估值赴美上市的计划。知情人士称，这家由丰田汽车支持的初创公司将寻求通过私募融资，估值为120亿美元；



> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [三星下一代手机芯片由AI来设计，EDA行业老大提供技术 | 量子位](https://mp.weixin.qq.com/s/oHz8y3LkR2glGHw6kL9Aow)  
摘要：据外媒《连线》的报道，三星将使用新思科技（Synopsys）提供的AI功能——DSO.ai——来设计下一代Exynos处理器。  
新思科技是全球最大的芯片设计软件（EDA）供应商之一，这家公司的董事长表示，DSO.ai是第一个用于处理器设计的商业AI软件。DSO.ai对设计速度的提升效果明显，新思科技说，这项工具在一些情况下将芯片频率提高了18%，功耗降低了21%，同时将工程时间从六个月缩短到了一个月。  
而且AI还会不断自学提高能力，从一个项目中获得的经验会被保留下来，用于未来的芯片设计工作。一家EDA厂商Cadence也与近期推出了AI设计工具。国外芯片软件技术遍地开花，而中国芯片厂商则面临着尴尬的局面。由于市场上三大EDA软件公司Synopsys、Cadence、Mentor均来自美国，且占据着中国95%的市场份额。  
- []()  
摘要：壁仞科技近日正式宣布，李新荣（Allen Lee）先生加入壁仞科技，出任联席CEO，专注组织，管理及产品设计端。李新荣先生的加入将会进一步加强壁仞科技的团队实力。  
李新荣先生在GPU领域拥有超过30年的丰富经验，加入壁仞科技之前在AMD就职15年，担任全球副总裁、中国研发中心总经理，负责AMD大中华区的研发建设和管理工作。在任期间，他一手构建了一个规模达数千人的研发团队，并实现了团队研发能力从单项目到覆盖“端到端”完整项目流程的重大突破。李新荣先生毕业于美国密苏里大学并获得电子工程硕士学位。  
目前，壁仞科技的团队规模已超500人，首款产品的研发也已顺利进入尾声，并将按计划于今年第三季度开始流片。  



## 论文

- [Only Train Once：微软、浙大等研究者提出剪枝框架OTO，无需微调即可获得轻量级架构 | 机器之心](https://mp.weixin.qq.com/s/_uUtDFL7lhxTkVR_Sl-VPQ)  
链接：https://arxiv.org/pdf/2107.07467.pdf  
摘要：来自微软、浙江大学等机构的研究者提出了一种 one-shot DNN 剪枝框架，无需微调即可从大型神经网络中得到轻量级架构，在保持模型高性能的同时还能显著降低所需算力。该研究的主要贡献概括如下：  
    1. One-Shot 训练和剪枝。研究者提出了一个名为 OTO（Only-Train-Once）的 one-shot 训练和剪枝框架。它可以将一个完整的神经网络压缩为轻量级网络，同时保持较高的性能。OTO 大大简化了现有剪枝方法复杂的多阶段训练 pipeline，适合各种架构和应用，因此具有通用性和有效性。  
    2. Zero-Invariant Group（ZIG）。研究者定义了神经网络的 zero-invariant group。如果一个框架被划分为 ZIG，它就允许我们修剪 zero group，同时不影响输出，这么做的结果是 one-shot 剪枝。这种特性适用于全连接层、残差块、多头注意力等多种流行结构。  
    3. 新的结构化稀疏优化算法。研究者提出了 Half-Space Stochastic Projected Gradient（HSPG），这是一种解决引起正则化问题的结构化稀疏的方法。研究团队在实践中展示并分析了 HSPG 在促进 zero group 方面表现出的优势（相对于标准近端方法）。ZIG 和 HSPG 的设计是网络无关的，因此 OTO 对于很多应用来说都是通用的。  
实验结果。利用本文中提出的方法，研究者可以从头、同时训练和压缩完整模型，无需为了提高推理速度和减少参数而进行微调。在 VGG for CIFAR10、ResNet50 for CIFAR10/ImageNet 和 Bert for SQuAD 等基准上，该方法都实现了 SOTA 结果。
- [MicroNets：更小更快更好的MicroNet，三大CV任务都秒杀MobileNetV3 | 我爱计算机视觉](https://mp.weixin.qq.com/s/8Veob1WRd3S-CnOdeedfRQ)  
摘要：本文旨在以极低的计算成本解决性能大幅下降的问题。作者发现有两个因素是可以有效提高精度的，分别是：稀疏连通性和动态激活函数。前者避免了网络宽度的大幅度缩减的危害，而后者则减轻了网络深度缩减的危害。这其中，作者首先证明了在给定的计算预算的情况下，降低节点连通性以扩大网络宽度为提供了一个很好的权衡。其次，依赖于改进的层非线性来弥补减少的网络深度，这决定了整个网络的非线性 。这两个因素激发了更有效卷积和激活函数的设计。  
技术上，作者提出了微分解卷积（micro-factorized convolution），它将卷积矩阵分解成低秩矩阵，以将稀疏连通性整合到卷积中，即Micro-Factorized convolution，其通过在pointwise和depthwise卷积上的低秩近似来实现通道数和输入/输出连接之间的平衡。  
此外，作者还提出了一个新的动态激活函数，称为Dynamic Shift Max，通过最大化输入特征图和圆形通道位移之间的多次动态融合来提高非线性，动态地融合了连续的通道组，增强节点连接性和非线性，以弥补深度的减少。  
基于这两个操作，作者提出一个新的网络系列：MicroNet，在低FLOP状态下，它取得了显著的性能提高。MicroNet对三个CV任务（图像分类、目标检测和人体姿态估计）都实现了明显的改进。例如，在12M FLOPs的约束下，MicroNet在ImageNet分类上达到了59.4%的Top-1准确率，比MobileNetV3高出9.6%。  


## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。


## 博文


- [计算图替代——一种DNN框架计算图优化方法 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/vcVJ3bYLoCv2UTgHzBjLuw)  
标题：Optimizing DNN Computation Graph using Graph Substitutions  
链接：http://www.vldb.org/pvldb/vol13/p2734-fang.pdf  
摘要：在DNN中每一回合的推理或训练中的每一次迭代通常可以表示为计算图，通过计算图优化可以提高DNN训练和推理的速度。目前主流的框架Tensorflow供了图优化器的API、TVM采用Op Fusion在内的多种计算图优化手段进行加速计算。  
**本文将主要介绍computation graph substitution优化方法**。计算图**替代就是找到另外一个计算图在功能上等效替代当前的计算图，在替代的同时可以减小计算时间以及计算量**。  
从现有的论文来看，**计算图替代可以起到一定的优化计算效果，需要将图形级和算子级优化结合起来**。这种联合优化具有挑战性，因为这两个问题都涉及到庞大且复杂的搜索空间，而一个级别的优化会影响另一个级别的搜索空间。未来研究方向应该是在减小搜索空间的同时进行最大限度的图替代，并且将计算图替代优化与其他优化方法结合，这样会给DNN框架优化计算带来最大的收益。 
