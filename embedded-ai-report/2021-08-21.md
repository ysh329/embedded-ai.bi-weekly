---
layout: default
---

# 嵌入式AI简报 (2021-08-05)：ncnn适配国产CPU/谷歌手机芯片/旷视YOLOX/OpenAI Triton/用MLIR优化GEMM/多GPU通信优化


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：

好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- pony.ai: 传小马智行暂缓赴美上市。公司回应称：公司并未确认过上市计划抑或上市时间线，已经暂缓了通过SPAC以120亿美元估值赴美上市的计划。知情人士称，这家由丰田汽车支持的初创公司将寻求通过私募融资，估值为120亿美元；
- 三星：
- AMD：Mercury Research 的最新数据显示，今年 2 季度，AMD 已成功占据 x86 处理器市场的 22.5% 份额。这是自 2007 年以来的最高纪录、并且接近 2006 年 25.3% 的历史峰值。不过英特尔的份额依然高达 77.5%，而威盛（VIA）等少数 x86 处理器厂家的全球份额几乎可以忽略不计。这份报告还提供了更细致的市场划分统计，可知 AMD 台式 CPU 的份额略有下滑，从 2021 年 1 季度的 19.3%、降到了 2 季度的略高于 17% ；


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [三星下一代手机芯片由AI来设计，EDA行业老大提供技术 | 量子位](https://mp.weixin.qq.com/s/oHz8y3LkR2glGHw6kL9Aow)  
摘要：据外媒《连线》的报道，三星将使用新思科技（Synopsys）提供的AI功能——DSO.ai——来设计下一代Exynos处理器。  
新思科技是全球最大的芯片设计软件（EDA）供应商之一，这家公司的董事长表示，DSO.ai是第一个用于处理器设计的商业AI软件。DSO.ai对设计速度的提升效果明显，新思科技说，这项工具在一些情况下将芯片频率提高了18%，功耗降低了21%，同时将工程时间从六个月缩短到了一个月。  
而且AI还会不断自学提高能力，从一个项目中获得的经验会被保留下来，用于未来的芯片设计工作。一家EDA厂商Cadence也与近期推出了AI设计工具。国外芯片软件技术遍地开花，而中国芯片厂商则面临着尴尬的局面。由于市场上三大EDA软件公司Synopsys、Cadence、Mentor均来自美国，且占据着中国95%的市场份额。  
- []()  
摘要：壁仞科技近日正式宣布，李新荣（Allen Lee）先生加入壁仞科技，出任联席CEO，专注组织，管理及产品设计端。李新荣先生的加入将会进一步加强壁仞科技的团队实力。  
李新荣先生在GPU领域拥有超过30年的丰富经验，加入壁仞科技之前在AMD就职15年，担任全球副总裁、中国研发中心总经理，负责AMD大中华区的研发建设和管理工作。在任期间，他一手构建了一个规模达数千人的研发团队，并实现了团队研发能力从单项目到覆盖“端到端”完整项目流程的重大突破。李新荣先生毕业于美国密苏里大学并获得电子工程硕士学位。  
目前，壁仞科技的团队规模已超500人，首款产品的研发也已顺利进入尾声，并将按计划于今年第三季度开始流片。  



## 论文

- [Only Train Once：微软、浙大等研究者提出剪枝框架OTO，无需微调即可获得轻量级架构 | 机器之心](https://mp.weixin.qq.com/s/_uUtDFL7lhxTkVR_Sl-VPQ)  
链接：https://arxiv.org/pdf/2107.07467.pdf  
摘要：来自微软、浙江大学等机构的研究者提出了一种 one-shot DNN 剪枝框架，无需微调即可从大型神经网络中得到轻量级架构，在保持模型高性能的同时还能显著降低所需算力。该研究的主要贡献概括如下：  
    1. One-Shot 训练和剪枝。研究者提出了一个名为 OTO（Only-Train-Once）的 one-shot 训练和剪枝框架。它可以将一个完整的神经网络压缩为轻量级网络，同时保持较高的性能。OTO 大大简化了现有剪枝方法复杂的多阶段训练 pipeline，适合各种架构和应用，因此具有通用性和有效性。  
    2. Zero-Invariant Group（ZIG）。研究者定义了神经网络的 zero-invariant group。如果一个框架被划分为 ZIG，它就允许我们修剪 zero group，同时不影响输出，这么做的结果是 one-shot 剪枝。这种特性适用于全连接层、残差块、多头注意力等多种流行结构。  
    3. 新的结构化稀疏优化算法。研究者提出了 Half-Space Stochastic Projected Gradient（HSPG），这是一种解决引起正则化问题的结构化稀疏的方法。研究团队在实践中展示并分析了 HSPG 在促进 zero group 方面表现出的优势（相对于标准近端方法）。ZIG 和 HSPG 的设计是网络无关的，因此 OTO 对于很多应用来说都是通用的。  
实验结果。利用本文中提出的方法，研究者可以从头、同时训练和压缩完整模型，无需为了提高推理速度和减少参数而进行微调。在 VGG for CIFAR10、ResNet50 for CIFAR10/ImageNet 和 Bert for SQuAD 等基准上，该方法都实现了 SOTA 结果。


## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。


## 博文


- [计算图替代——一种DNN框架计算图优化方法 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/vcVJ3bYLoCv2UTgHzBjLuw)  
标题：Optimizing DNN Computation Graph using Graph Substitutions  
链接：http://www.vldb.org/pvldb/vol13/p2734-fang.pdf  
摘要：在DNN中每一回合的推理或训练中的每一次迭代通常可以表示为计算图，通过计算图优化可以提高DNN训练和推理的速度。目前主流的框架Tensorflow供了图优化器的API、TVM采用Op Fusion在内的多种计算图优化手段进行加速计算。  
**本文将主要介绍computation graph substitution优化方法**。计算图**替代就是找到另外一个计算图在功能上等效替代当前的计算图，在替代的同时可以减小计算时间以及计算量**。  
从现有的论文来看，**计算图替代可以起到一定的优化计算效果，需要将图形级和算子级优化结合起来**。这种联合优化具有挑战性，因为这两个问题都涉及到庞大且复杂的搜索空间，而一个级别的优化会影响另一个级别的搜索空间。未来研究方向应该是在减小搜索空间的同时进行最大限度的图替代，并且将计算图替代优化与其他优化方法结合，这样会给DNN框架优化计算带来最大的收益。 
