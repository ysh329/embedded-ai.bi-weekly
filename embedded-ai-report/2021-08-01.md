---
layout: default
---

# 嵌入式AI简报 (2021-08-01)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：


好了，先是一些热身ヽ(✿゜▽゜)ノ：

- 高通：正研发一款 SM8450 的芯片代号为 Waipio ，将是骁龙888真正的迭代更新即骁龙895或898，将采用三星4nm工艺，CPU 由 X2@3.09GHz + 3xA710 + 2xA510@high + 2xA510@low 组成；
- 台积电：**Q2财报发布后，股价大跌5.5%，市值蒸发2300亿**。原因是失去华为麒麟芯片的代工订单导致利润下滑。同时因为失去华为抢占产能，苹果可以选择三星和台积电，打压台积电订单价格，导致台积电利润进一步下滑。今年 2 月董事会通过将斥资 28 亿美元扩建月产4万片的28纳米产能的南京厂，但美方施压下，预计明年下半年开始量产；
- 龙芯中科：**首款自主架构 LoongArch 的处理器 3A5000 发布，LoongArch 从顶层架构，到指令功能和ABI标准等全自主，满足兼容生态、跨指令平台应用的需求**，处理器主频2.3GHz-2.5GHz，4核心，每核心为 64 位超标量 GS464V 自主微结构，包含4个定点单元、2个256位向量运算单元和2个访存单元；国内第三方测试其在GCC编译环境下运行SPEC CPU2006的定点、浮点单核Base分值均达到26分以上，四核分值达到80分以上；
- Nvidia：收购 Arm 再延迟。但 **Arm CEO Simon Segars 说：“Arm 没有进行 IPO 计划，我们 100% 专注于完成这笔交易”，此前他曾公开反对上市，认为 Arm 需要更多投资来扩展到数据中心和人工智能领域**，而这作为上市公司是不可能的。本月初，Segars 先生写道：“**Arm 和 Nvidia 的合并比 IPO 更好**。引领人工智能所需的投资水平将是前所未有的”；
- Arm: 推出PlasticArm柔性芯片，早在 2015 年时由于技术限制仅是概念，现已准备规模生产，成本低于同类硅芯片；
- GlobalFoundries：CEO Tom Caulfield表示，该公司坚持明年IPO计划，有关成为英特尔公司收购目标的报道只是猜测。格芯是阿布扎比财团投资部门的一家芯片制造商；
– Imagination：**宣布任命白农（Wallace Pai）担任中国区董事长**，深化中国市场战略，在战略、销售与业务合作方面有丰富经验，后者曾在中芯国际/格芯担任副总或高管，在三星/谷歌/高通/Cadence/Intel担管理等；
- 苹果：正在加快自研芯片的进度，其打算在2022年完成旗下电脑产品都使用自家处理器的计划。**下一代产品 M2 将支持更多雷雳通道，有更多 CPU核心、GPU核心**，多外接显示器支持，**包括10核CPU（8个高性能核心，2个高能效核心），以及16核或32核 GPU 设计**，最高支持64GB内存；
- 3dfx Interactive：将在时隔 20 年后重回市场，定于 8 月 5 日对外公布重要消息；
- 芯原：与浙江大学-芯原智能图形处理器联合研究中心正式揭牌，目前已经在GPU的空间架构、光线追踪等方面取得一些进展；
- Canalys：发布全球Q2智能手机市场份额的报告，**小米市场份额 17%，智能手机销量超越了苹果，晋升全球第二**；
- Intel：在欧盟多国游说，拟投资 200 亿美元建芯片工厂，希望获得财政和政治支持。预计将在今年年底完成 8 个选址工作；宣布其旗下的工厂将开始制造高通芯片，并公布了公司有史以来最详细的制程工艺和封装技术路线图，希望在 2025 年前赶上台积电、三星电子，**将为 AWS、高通代工芯片，AWS 将成为首个使用英特尔代工服务（IFS）封装解决方案的客户，高通也将采用 Intel 20A 制程工艺技术**。英特尔还将采用下一代高数值孔径（High-NA）EUV 技术；
- Cadence Design：CEO Lip-Bu Tan 将于 2021 年 12 月 15 日转任执行董事长，届时 Anirudh Devgan 将接任 CEO 职位。Devgan 一直负责监督所有研发、销售和现场工程以及企业战略、营销和业务发展团队，当中还包括并购，并推动了技术和产品路线图，是公司当前行业领先解决方案背后多项突破性创新的架构师；
- AMD：**2021Q2 财报营业额同比增长 99% 毛利润增至 48%**，营业额为 38.5 亿美元，经营收入为 8.31 亿美元，净收入为 7.1 亿美元，摊薄后每股收益为 0.58 美元。非 GAAP 经营收入为 9.24 亿美元，净收入为 7.78 亿美元，摊薄后每股收益为 0.63 美元。**主要得益于计算与图形事业部及企业、嵌入式和半定制事业部的较高营业额**；
- 腾讯: **在招聘官网出现多个芯片研发岗位信息**，包括芯片架构师、芯片验证工程师、芯片设计工程师等，工作地点可选北京、上海、深圳等。腾讯相关人士回应称，**在特定的领域如 AI 加速和视频编解码有在做技术尝试**，非通用芯片；
- 芯驰：宣布完成近 10 亿元 B 轮融资。本轮融资由普罗资本旗下国开装备基金与云晖资本联合领投，董事长张强表示融资将用于更先进制程芯片的研发，实现更好的性能和功耗表现，**推动智能驾驶更快落地**。


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [MediaTek 发布全新移动计算平台 迅鲲™1300T | 联发科技](https://mp.weixin.qq.com/s/GO4WuuCSzE2pVOwjJYd0Hg)
摘要：联发科**新款移动计算平台迅鲲（Kompanio） 1300T**。采用台积电 6nm ，**4xaA78 + 4x@A55 + Mali-G77 MC9**，搭载 HyperEngine 3.0 游戏引擎。还集成**APU 3.0 AI单元**，提供支持智能语音(语音助手/超低功耗语音唤醒/多款音效框架)、视觉应用。首款搭载迅鲲 1300T 的平板电脑可能是 8 月 12 日将发布的荣耀平板 V7 Pro。  
- [谷歌 CEO 晒自研手机芯片 Tensor：首发于今年秋季谷歌旗舰手机 Pixel 6 和 6 Pro | 芯东西](https://mp.weixin.qq.com/s/q-Z2667Ud6CANiwW0vC7Vw)  
摘要：Pixel系列手机首次弃用高通骁龙芯片，有8个CPU核心，其中包括两个Cortex-A76核心和四个较小的Cortex-A55核心。在GPU上，Tensor可能会采用Arm代号“Borr”的Mali GPU设计或三星的Exynos软件组件，而Tensor的5G基带则可能选择高通的X60或X65。作为谷歌为Pixel量身定制的SoC，Tensor芯片尤其围绕AI运算和安全性进行优化，进一步增强计算摄影表现，并在语音命令、机器翻译、字幕和听写等功能方面提供更快响应。  
- [双核+ GPU 加持，华米科技黄山 2S 来了：智能可穿戴芯片进入全新时代 | 机器之心](https://mp.weixin.qq.com/s/KlPwyE32oZLkRvePwhF4xQ)  
摘要华米在合肥举行的发布会上，推出了新一代双核 RISC-V 架构可穿戴芯片黄山 2S、自研可穿戴操作系统 Zepp OS、血压测量引擎 Pump Beats 等多项新技术。该芯片中还集成了一颗 2.5D GPU，图形加速性能比上一代提升 67%，可独立处理图形相关指令，让操作系统运行更加流畅。在实机演示中，它可以让手表上的 UI 界面最高达到 60Hz 刷新率。此外，黄山 2S 芯片搭载的卷积神经网络加速处理单元，可以迅速识别疾病类型；在处理房颤识别任务时，识别速度是纯软件计算的 26 倍。  
- [酷芯全新AR93xx系列AI SoC全球首发，携智慧解决方案即将亮相2021 WAIC | 酷芯微电子](https://mp.weixin.qq.com/s/-g6aEQdi60zTq959BUeIAw)  
摘要：首发全新一代 AR93XX 系列高性能 AI SoC，**集成自研 900 万像素 ISP 和 4T 算力 NPU 等 IP**，并展出与合作伙伴开发智慧解决方案。结合 4K@60fps 编解码，单核 CEVA XM6 高性能 DSP，4 核 ARM Cortex-A53，自研 SoC 架构，综合处理能力**宣称业界最优**。  
- [“智能计算产业技术创新联合体”成立，宣布全球首个开源神经网络处理器指令集架构 | Arm中国](https://mp.weixin.qq.com/s/b5FCfzTgoBo9VbsqVPvZHw)  
摘要：Arm China: 与清华集成电路学院、中兴微电子、TCL集团工研院、全志科技、瑞芯微电子、长安汽车研究院、前海七剑等多家企业和机构共同发起的智能计算产业技术创新联合体（Open NPU Innovation Alliance，简称ONIA）成立“智能计算产业技术创新联合体”，并**正式宣布全球首个开源神经网络处理器指令集架构（NPU ISA）**。今后将以标准协作等方式制定、批准和维护开源NPU ISA，并为未来的规范设定方向，构建由中国本土发起、以全球领先技术为标准的智能计算产业生态，实现NPU处理器创新和智能计算的持续演进；


## 论文


- [MobiSys 2021] [可高效、准确地预测模型推理时间的系统nn-Meter | 微软研究院AI头条](https://mp.weixin.qq.com/s/_axPjHPLCh1rgTqfCbDBUg)  
标题：[nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices](https://air.tsinghua.edu.cn/Uploads/UEditor/Files/20210709/6376145008525256118804429.pdf)  
链接：https://air.tsinghua.edu.cn/Uploads/UEditor/Files/20210709/6376145008525256118804429.pdf  
摘要：这篇来自微软亚洲研究院的论文获得了 MobiSys 2021 的最佳论文奖（Best Paper），并且成为本届大会中唯一个获得了 Artifact Evaluation 全部三个最高级别徽章的工作。  
深度神经网络（DNN）模型在实际部署中的延迟（推理时间）是决定模型是否可用的一个重要指标。然而，模型设计过程中对数以亿计的设计选项进行实际的部署和延迟评估会造成巨大的开销。因此，如何进行高效、准确的模型运行延迟预测对模型的设计至关重要。但现有技术缺乏对部署平台优化策略的理解以及对灵活多变模型架构的泛化性，所以无法做到准确的模型推理时间预测。  
针对上述问题，微软亚洲研究院异构计算组的研究员们提出并开发了nn-Meter 模型推理时间预测系统。该系统可高效、准确地预测 DNN 模型在不同边缘设备上的推理延迟，其关键思想是将整个模型划分为内核（kernel），即设备上的执行单元，然后执行内核级预测。  
与基准方法相比，nn-Meter 是唯一能够在各种设备上始终实现准确预测的方法。平均而言，nn-Meter 89.2% 的准确率明显优于 FLOPs (22.1%)、FLOPs+MAC(17.1%) 和 BRP-NAS (8.5%)。nn-Meter 在完整的包含26,000个模型的基准数据集上（表2） 的预测结果中，分别在移动 CPU 和 GPU 上实现了99.0%和99.1%的预测准确率。在 Intel VPU 上，nn-Meter 则可以在±10%的误差范围内达到83.4%的预测准确率。nn-Meter 建立在两个关键技术之上，从而可以准确预测不同模型在部署中的推理时间，以设计真正高效的模型：  
    - 内核检测：能够自动识别部署平台的优化策略，从而基于这些策略将模型分解为实际运行的内核，nn-Meter 会离线收集所有融合规则，对于在线模型预测，内核搜索算法则会将这些规则递归地应用于目标模型来找到所有内核。  
    - 自适应数据采样：从整个设计空间中有效地采样最有益的配置，以高效地构建准确的内核级延迟预测器。  
对于每个内核，nn-Meter 都会提取特征并预测其延迟，所有内核预测延迟之和则为整个模型的预测延迟。  
- [2107.12292] [JDAI-CV/CoTNet：京东AI开源最强 ResNet 变体 CoTNet，即插即用的视觉识别模块 | 极市平台](https://mp.weixin.qq.com/s/AUBtxtriRMi2lud8E7w5cA)  
摘要：本文是京东AI研究院梅涛团队在自注意力机制方面的探索，不同于现有注意力机制仅采用局部或者全局方式进行上下文信息获取，他们创造性的将Transformer中的自注意力机制的动态上下文信息聚合与卷积的静态上下文信息聚合进行了集成，提出了一种新颖的Transformer风格的“即插即用”CoT模块，它可以直接替换现有ResNet架构Bottleneck中的卷积并取得显著的性能提升。无论是ImageNet分类，还是COCO检测与分割，所提CoTNet架构均取得了显著性能提升且参数量与FLOPs保持同水平。比如，相比EfficientNet-B6的84.3%，所提SE-CoTNetD-152取得了84.6%同时具有快2.75倍的推理速度。  
    1. 技术上来讲，CoT 模块首先通过卷积对输入keys进行上下文信息编码得到关于输入的静态上下文表达；进一步将编码keys与输入query进行拼接并通过两个连续卷积学习动态多头注意力矩阵；所得注意力矩阵与输入values相乘即可得到关于输入的动态上下文表达。  
    2. CoTNet-50直接采用CoT替换Bottlenck中的卷积；类似的，CoTNeXt-50采用CoT模块替换对应的组卷积，为获得相似计算量，对通道数、分组数进行了调整：CoTNeXt-50的参数量是ResNeXt-50的1.2倍，FLOPs则是1.01倍。


## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。


- [mindspore-ai/mindspore: 发布1.3版本，打造无所不在的智能，诠释可以信赖的开源 | MindSpore](https://mp.weixin.qq.com/s/9N_Ib8ZbgbVVEn-7R8zpKg)  
摘要：在这个版本中为大家带来了全新的MindSpore Federated能力，解锁了支撑盘古千亿稠密大模型的众多关键特性、以及面向更多类型硬件的推理优化、图算融合、简易部署等。  
- [taichi-dev/quantaichi：99行代码实现冰雪奇缘特效的「太极」再进化，胡渊鸣团队、快手等联合打造 | 机器之心](https://mp.weixin.qq.com/s/vJFOziFu2Dre6QQbXeAtRA)  
论文：https://yuanming.taichi.graphics/publication/2021-quantaichi/quantaichi.pdf  
项目：https://yuanming.taichi.graphics/publication/2021-quantaichi/  
代码：https://github.com/taichi-dev/quantaichi  
摘要：现代动画电影（包括《冰雪奇缘》等），经常使用基于物理的动画生产特效，丰富感官的体验。基于粒子的表示是其中常用的方法。场景越大，粒子就越多。比如，要模拟一个 300 米长的溃坝场景中的水，可能会需要数千万粒子，而这些粒子的存储需要大量显存。比如说，如果需要96GB的显存，则需要购置大量高端显卡，如 4 块 NVIDIA Quadro P6000 GPU。  
针对这一现状，快手、麻省理工、浙大、清华的研究者进行了物理编译器自动量化方面的研究，提出了一套用于量化模拟的新的语言抽象和编译系统——QuanTaichi。它可以使用低精度量化的数字数据类型并将其打包（packing）以表示模拟状态，从而减少了内存空间和带宽消耗。有了这项技术的加持，高精度的物理模拟只需要一块 GPU 就能实现。  
QuanTaichi 的实现基于 MIT CSAIL 胡渊鸣等人之前提出的「太极（Taichi）」编程语言和编译器。太极技术已经让快手成为首个推出实时液体及烟雾模拟动态效果的短视频和直播平台，行业首发了「别哭鸭」、「我要去潜水」、「火焰超能力」等特效。其中，「圣诞快乐」魔法表情成为爆款，有 74 万用户拍摄并上传了视频，大约有两千多万用户观看了太极支持的这款魔法表情。  
- [KaiyuYue/torchshard：训练大模型也不怕，轻量级TorchShard库减少GPU内存消耗，API与PyTorch相同 | 机器之心](https://mp.weixin.qq.com/s/IPO_FhFFtg7ajaVyEVrA0w)  
摘要：训练大模型时，如何优雅地减少 GPU 内存消耗？你不妨试试这个 TorchShard 库，兼具模型并行与数据并行等特点，还具有与 PyTorch 相同的 API 设计。马里兰大学帕克分校计算机科学系的研究者 Kaiyu Yue 开源了一个工具TorchShard，这是一个轻量级的引擎，用于将 PyTorch 张量切片成并行的 shard。当模型拥有大量的线性层（例如 BERT、GPT）或者很多类（数百万）时，TorchShard 可以减少 GPU 内存并扩展训练规模，它具有与 PyTorch 相同的 API 设计。  
TorchShard 是对模型并行单元（mpu）的彻底重写，是 Megatron-LM 核心。最重要的是，TorchShard 具有与 PyTorch 相同的 API 设计，这意味着所有的子类和子函数都保持与 PyTorch 相同。除此之外，TorchShard 还支持与 DDP 一起使用时的各种特性，保存和加载 shard checkpoints，初始化 shard 参数，以及跨多台机器和 GPU 处理张量。  
- [tensorflow/tfjs-models：实时检测17个人体关键点，谷歌SOTA姿态检测模型，手机端也能运行 | 机器之心](https://mp.weixin.qq.com/s/2Mk_FQoTB1wLmkyk5X865w)  
代码：https://github.com/tensorflow/tfjs-models/tree/master/pose-detection  
摘要：近日，来自谷歌的研究者更新了用于实时姿态检测的项目，该项目包含 3 种 SOTA 模型，其中 MoveNet 模型可检测人体 17 个关键点、并以 50+ fps 在电脑和手机端运行；BlazePose 可检测人体 33 个关键点；PoseNet 可以检测人体多个姿态，每个姿态包含 17 个关键点。
- [kingsoft-wps/KSAI-Toolkits：AI加持的WPS来了：金山开源全球首个办公DL框架KSAI-Lite | 机器之心](https://mp.weixin.qq.com/s/yxbRZH3Wlql09ZQStelHpw)  
代码：https://github.com/kingsoft-wps/KSAI-Toolkits  
摘要：金山办公还发布了一款人工智能深度学习推理框架 KSAI-lite，这是一款免费、开源、跨多个终端的全新工具，适配国内外主流软硬件平台，在 OCR、机器翻译、智能校对等落地场景上为开发者们带来了新选择。KSAI-lite 面向通用性、高性能、轻量和专业性四个目标构建。  
在技术实践中，金山的开发团队在多框架支持、软硬件适配、性能、功耗、内存等方面都进行了优化。在 KSAI-lite 中首个开源的是 OCR 模型，其支持移动端设备的离线识别，模型和库文件共计不到 9MB。该模型在文本检测、文本分类和文本识别上都表现出了业内第一梯队的性能。据介绍，KSAI-lite 框架底层基于 TensorFlow。  


## 博文


- [基于MLIR实现GEMM编译优化 | 壁仞科技研究院](https://mp.weixin.qq.com/s/A1h4pJSJ8VF97DrZksNULg)  
摘要：GEMM(General Matrix Multiplication)即通用矩阵乘法运算，由于其计算行为具有一定的复杂性以及规律性，是编译算法研究的绝佳场景。MLIR是近期非常热门的一个编译器软件框架，是工业界及科研界研究的一个热点，其提供了一套灵活的软件基础设施，对中间表达式（IR）及其相互之间的转换进行规范的管理，是一个非常友好的编译器开发平台。  
本文主要介绍了矩阵乘法运算在MLIR编译器框架实现的主要过程及内容，以及其在优化算法的多层次实现，以及接入已有优化算法的能力等方面的优势。MLIR编译框架，为编译器的开发者，在多层中间表达转换以及算法优化等方面提供强大的基础设施支持，降低开发编译器的门槛。希望通过本文，可以让读者对MLIR的工程实现过程更加清晰一些，从中得到一点启发。  
- [深度学习算子优化-FFT | 旷视研究院](https://mp.weixin.qq.com/s/WffoKy-zd44270uDQM16jg)  
摘要：DFT/FFT 在深度学习领域也有延伸应用。比如利用 FFT 可以降低卷积计算量的特点，FFT_Conv 算法也成为常见的深度学习卷积算法。FFT_conv 的核心计算模块还是针对小图的 DFT 运算，辅以多线程，进一步提升 FFT_Conv 的计算效率。本文我们就来探究一下频域算法的原理和优化策略。  
- [计算图替代——一种DNN框架计算图优化方法 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/vcVJ3bYLoCv2UTgHzBjLuw)  
摘要：在DNN中每一回合的推理或训练中的每一次迭代通常可以表示为计算图（Computational graphs：a common way to represent programs in deep learning frameworks），通过计算图优化可以提高DNN训练和推理的速度。目前主流的框架Tensorflow、Pytorch、TVM等都是同时采用多种计算图优化手段进行加速计算，Tensorflow提供了图优化器的API，用户可以直接调用；TVM采用Op Fusion（Operator fusion：combine multiple operators together into a single kernel without saving the intermediate results back into global memory）等方法来进行计算优化。  
本文将主要介绍computation graph substitution优化方法。计算图替代就是找到另外一个计算图在功能上等效替代当前的计算图，在替代的同时可以减小计算时间以及计算量。从现有的论文来看，计算图替代可以起到一定的优化计算效果，需要将图形级和算子级优化结合起来。这种联合优化具有挑战性，因为这两个问题都涉及到庞大且复杂的搜索空间，而一个级别的优化会影响另一个级别的搜索空间。未来研究方向应该是在减小搜索空间的同时进行最大限度的图替代，并且将计算图替代优化与其他优化方法结合，这样会给DNN框架优化计算带来最大的收益。  
- [AI框架基础技术之深度学习中的通信优化 | 商汤学术](https://mp.weixin.qq.com/s/4hDUJonD2szCx6420T6bWw)  
摘要：日益增加的算力需求将模型的训练推向了多 GPU 训练甚至更大规模的分布式训练。而分布式训练的加速效果和可扩展性，很大程度上受限于节点之间的通信。正文分为四个章节，分别介绍在深度学习模型训练中常见的通信方式、深度学习框架中通信的实现、通信的优化策略，以及如何在模型搭建中进行通信优化。  



