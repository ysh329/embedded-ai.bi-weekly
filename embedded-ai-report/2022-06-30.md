---
layout: default
---

# 嵌入式AI简报 (2022-06-30)：  


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：【新闻】；【论文】；【开源】；【博文】。


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 台积电：2nm N2技术路线图亮相，计划2025年生产，承诺让芯片设计人员在同功率和晶体管数量下将性能提升 10% 至 15%，或在相同频率和复杂度下将功耗降低 25% 至 30%。广泛使用 EUV 光刻技术，并引入了 GAAFET（台积电称之为纳米片晶体管）以及背面供电；预计台南的生产中心再建四座价值 100 亿美元的工厂，用于制造 3 nm芯片，覆盖 Apple SoC、 A 系列芯片；
Imagination：推出其首款32位实时嵌入式IMG RISC-V CPU- RTXM-2200，128KB的L1 I-Cache和D-Cache，支持fp32/bf16类型，适用于低成本AI部署；其GPU获得ADAS和HMI应用的ISO 26262功能安全认证；
- 三星：六月中旬副董事长会见了 ASML CEO 讨论了高端芯片设备合作疑抢购EUV光刻机；宣布基于 GAA （全环栅晶体管） 量产3nm 工艺，或将是世界上第一个迈入 GAA 结构的晶圆厂；目前在开发下一代旗舰SoC Exynos 2300，型号S5E9935代号为“Quadra”，采用三星3nm GAA工艺，CPU为ARM最新架构，GPU为AMD最新的Radeon GPU，或将于2023年上半年Galaxy S23系列首发；
- 龙芯：龙芯中科登陆科创板，其下一代3A6000预计2023年发布，采用12nm并有大幅度架构升级，单核SPEC CPU 2006定点/浮点base分值（GCC）从26/28分提高到35/45分，内存双通道DDR4的Stream带宽(峰值51.2GBps)也将从25GBps提高到38GBps；
- RISC-V：架构改进，宣布2022年的首批四项规格和扩展的批准：高效跟踪（E-Trace）、主管二进制接口（SBI）、统一可扩展固件接口（UEFI）规格，及Zmmul纯乘法扩展有利于简单的FPGA软核；
- 联发科：发布旗下首款支持5G毫米波的移动平台——天玑1050，6nm工艺，CPU为A78@2.5GHz\*2+A55\*6，GPU为mali-G610，APU为APU550提升AI相机功能等；CounterPoint公布了2022Q1全球智能手机SoC市场统计报告，联发科以天玑系列，以38％的市场份额再次拿下第一。已连续七个季度全球第一，目前已经覆盖高，中低端（天玑700/900），同时不少相应终端正在筹备中。按出货量计算，高通份额为30%位居第二，苹果15％，紫光展锐11％，三星5％，海思1％；
- AMD：支持芯片定制，向第三方Chiplet打开大门。允许客户如早先的索尼，微软游戏主机，可以在紧凑的芯片封装中实现多个裸片（也称为chiplet或compute tiles ）。AMD 已经在使用tiles，但现在欢迎第三方制造加速器或汽车等其他芯片，以将其与 x86 CPU 和 GPU ，AI加速器一起包含在 其2D 或 3D 封装中；
- 高通&Arm：高通CEO安蒙表示有意愿联合其它公司，在ARM IPO时投资入股或者直接收购，同时组成联合财团直接收购ARM。Intel CEO此前表示，公司会支持ARM基石投资的举措，而安蒙这一表态，无疑会给ARM基石投资者提供新的动力；
- 高通：第2代骁龙8将在今年年底登场，大核变多，将采用台积电4nm工艺，且X3\*1+A720\*2+A710\*2+3的八核心架构设计，骁龙8+为1\*X2+3\*AA710+4的架构，2代的GPU为Adreno 740；
- Arm：英国正尽一切可能让 Arm 在伦敦上市，英国正全力以赴确保Arm在伦敦上市。自 4 月以来，英国经济官员一直在动员其母公司软银集团将Arm在伦敦证券交易所上市；
- MIT：MIT 工程师构建类似乐高的 AI 芯片。MIT 工程师采用类似乐高的设计，创建出一款可堆叠、可重新配置的人工智能（AI）芯片。这种芯片构件可使设备保持最新状态，同时减少电子浪费。新设计使用光而不是物理线来通过芯片传输信息。因此可根据需要添加任意数量的计算层和光、压力甚至气味传感器。研究人员称其为类似乐高的可重构 AI 芯片，因为它根据层的组合具有无限的可扩展性。在新芯片设计中，研究人员将图像传感器与人工突触阵列配对，训练每个突触阵列识别某些字母——在本例中为 M、I 和 T，团队在每个传感器和人工突触阵列之间制造了一个光学系统，以实现层之间的通信，而无需物理连接，因此能以想要的方式自由地堆叠和添加芯片；
- Android：Android 13 进入第三个 beta 版，谷歌正式推出了 Android 13 的第三个测试版本，并表示已经达到了“平台稳定”，这表明谷歌已经锁定了主要更新，接下来的工作就是打磨细节，修复 BUG，优化性能了。它也比 Android 12 提前了整整两个月达到了这个里程碑。


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [谷歌官方回应：我们没有放弃TensorFlow，未来与JAX并肩发展 | 机器之心](https://mp.weixin.qq.com/s/SGVThtwKj2jl3YFRuqzCfw)  
摘要：最近大家也看到不少有关谷歌的一些部门更倾向于JAX，而非TF，JAX的特点主要有：1. NumPy加速器，让模型很轻松在GPU和TPU上运行；2. XLA（Accelerated Linear Algebra）就是加速线性代数，一个优化编译器。JAX建立在XLA之上，大幅提高了JAX计算速度的上限；3. JIT。研究人员可使用XLA将自己的函数转换为实时编译（JIT）版本，用户可以用函数修饰符将将计算速度提高几个数量级。此外，JAX与Autograd完全兼容，支持自动差分，通过grad、hessian、jacfwd和jacrev等函数转换，支持反向模式和正向模式微分，并且两者可以任意顺序组成。以上也是JAX的优点。  
当然JAX也有一些不友好的地方：1.未针对CPU计算做充分优化；2.未形成像TensorFlow那样完整的基础生态以及产品；3. Debug困难；4. 不支持Windows系统；5. 不是一个深度学习框架，没有数据加载模块，需要嵌入TensorFlow或PyTorch中使用。
本文，谷歌发声：TensorFlow 不是谷歌的一枚「弃子」，将会继续开发。我们的愿景是创建一个有凝聚力的生态系统，研究人员和工程师可以利用系统组件进行研究，而不管它们起源于哪个框架。我们已经在 JAX 和 TensorFlow 互操作性方面取得了长足进步，特别是 jax2tf 的开发。  
- [高通AI软件栈：推动OEM厂商和开发者的AI开发，已商用上市 | 高通中国](https://mp.weixin.qq.com/s/e10-dPe0OqZBNGpP8AD8Hg)  
摘要：该AI软件栈是面向OEM厂商和开发者的一套完整的AI解决方案，覆盖智能手机、汽车、XR、计算、物联网和云平台。  
AI软件栈支持包括TensorFlow、PyTorch和ONNX在内的不同AI框架与主流runtimes，以及开发者库与服务、系统软件、工具和编译器，AI软件栈产品组合还支持一系列工具套件，包括高通AI模型增效工具包（AIMET）、AI开发图形用户界面（GUI）、用于增强量化与优化的模型分析器以及神经网络架构搜索（NAS）。  
2021骁龙技术峰会上，高通和Google Cloud宣布将Google Cloud Vertex AI NAS集成至高通神经网络处理SDK，赋能OEM厂商和生态系统打造高效的边缘侧体验。其SDK仍然是OEM厂商和开发者在高通技术公司各类产品上运行神经网络的关键。更多见：https://www.qualcomm.com/products/technology/artificial-intelligence/ai-stack  
- [苹果M2处理器发布：第二代5nm工艺 8核CPU+10核GPU | 安兔兔](https://mp.weixin.qq.com/s/kb13gKdbzuj64KG-lmVtcA) 
摘要：CPU方面，依然是8核心，4大4小，其中4个性能核心还是超宽执行架构，每核心有192KB指令缓存、128KB数据缓存，共享16MB缓存。4个能效核心，同样是宽执行架构，每核心128KB指令缓存、64KB数据缓存，共享4MB缓存，官方数据显示CPU速度整体提高18%。  
GPU集成10个核心，相比M1增加两个，速度提高35%，集成多达24GB LPDDR5统一内存，位宽128-bit，带宽超过100GB/s，比M1增加50％。得益于更大的缓存和更高的内存带宽，M2在同等功耗水平下的图形性能较M1提升最多达到25%，在最高功耗水平下的性能较M1芯片提升更可达35%。  
下一代神经引擎，每秒可以进行最多达15.8万亿次运算，支持8K H.264和HEVC视频的媒体引擎，能够同时播放多个4K和8K视频流，以及外接6K显示屏。  


## 论文  

- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness | 机器之心]()  
论文：https://arxiv.org/pdf/2205.14135.pdf  
摘要：通过减少 GPU 内存读取 / 写入，FlashAttention 的运行速度比 PyTorch 标准注意力快 2-4 倍，所需内存减少 5-20 倍。作者研究认为：应让注意力算法具有 IO 感知——即考虑显存级间的读写。现代 GPU 计算速度超过了内存速度，transformer 中的大多数操作都被内存访问所阻塞。IO 感知算法对于类似的内存绑定操作至关重要，这种重要性体现在当读写数据占据很大运行时——例如数据库连接、图像处理、数值线性代数等。然而，用于深度学习的常见 Python 接口，如 PyTorch 和 Tensorflow，不允许对内存访问进行细粒度控制。  
该研究提出了一种新的注意力算法 FlashAttention，它可以使用更少的内存访问来计算精确的注意力。FlashAttention 旨在避免从 HBM（High Bandwidth Memory）中读取和写入注意力矩阵。这需要做到：(i) 在不访问整个输入的情况下计算 softmax reduction；(ii) 在后向传播中不能存储中间注意力矩阵。  
作者通过 CUDA 实现了细粒度内存访问以实现 FlashAttention ，并将所有计算融合到一个 GPU 内核中。即使在需要重新计算导致 FLOPs 增加，但其运行速度更快，且使用更少的内存（序列长度线性），反而整体变快。归根结底是为大大减少了 HBM 访问量。  
- [meituan/YOLOv6: YOLOv6——精度与速度远超 YOLOv5 和 YOLOX 的新框架 | 美团技术团队](https://mp.weixin.qq.com/s/RrQCP4pTSwpTmSgvly9evg)  
代码：https://github.com/meituan/YOLOv6  
摘要：本文介绍了美团视觉智能部在目标检测框架方面的优化及实践经验，针对 YOLO 系列框架，在训练策略、主干网络、多尺度特征融合、检测头等方面进行了思考和优化，设计了新的检测框架-YOLOv6，初衷来自于解决工业应用落地时所遇到的实际问题：
    1. 更高效的 Backbone 和 Neck ：受到硬件感知神经网络设计思想的启发，基于 RepVGG style 设计了可重参数化（其结构在训练时有多分支拓扑，实际部署时可融合为单个 3x3 卷积）、更高效的骨干网络 EfficientRep Backbone 和 Rep-PAN Neck；
    2. 优化设计了更简洁有效的 Efficient Decoupled Head，维持精度同时，降低了一般解耦头带来的额外延时开销；
    3. 训练策略：采用Anchor-free 无锚范式，同时辅以 SimOTA 标签分配策略以及 SIoU 边界框回归损失来进一步提高检测精度。
在工业界常用的尺寸模型中：YOLOv6-nano 在 COCO 上精度可达 35.0% AP，在 T4 上推理速度可达 1242 FPS；YOLOv6-s 在 COCO 上精度可达 43.1% AP，在 T4 上推理速度可达 520 FPS。在部署方面，YOLOv6 支持 GPU（TensorRT）、CPU（OPENVINO）、ARM（MNN、TNN、NCNN）等不同平台的部署，极大地简化工程部署时的适配工作。  
- [PP-YOLOE: An evolved version of YOLO](https://arxiv.org/abs/2203.16250)  
论文：https://arxiv.org/abs/2203.16250  
摘要：百度飞桨团队发布了 PP-YOLOE，与其他 YOLO 系列算法相比，其具有更强的性能、更丰富灵活的配置方案以及更全硬件支持三大优势。YOLO 界再起波澜！mAP 51.4，149FPS，目标检测，一个就够了。  
- [MobileOne: 移动端仅需1ms的高性能骨干，你值得拥有 | AIWalker](https://mp.weixin.qq.com/s/crsRcY7dm6HJSd-QjcoUYg)  
摘要：MobileOne 是由Apple公司提出的一种基于iPhone12优化的超轻量型架构，在ImageNet数据集上以<1ms的速度取得了75.9%的Top1精度。  
其设计思路MobileOne ≈ MobileNetV1 + RepVGG + 训练Trick。作者以iPhone12平台为基准，从不同维度进行了"瓶颈"分析：发现参数多推理也可以跑得快，计算量大FLOPs也可以跑得快，那么换句话说，在移动端，延迟与FLOPs和参数量的相关性较弱，而在PC-CPU端，该相关性进一步弱化。  
性能上，对激活函数、block结构分别选择ReLU和SE模块，选择RELU是因为其他激活函数慢，选择SE是因为其单分支结构也更快。基于上述分析，MobileOne的核心模块基于MobileNetV1而设计，同时吸收了重参数思想。  
训练上，因小模型要更少正则，作者提出了Annealing的正则调整机制(可带来0.5%指标提升)；引入渐进式学习机制(可带来0.4%指标提升)；最后，作者还采用EMA机制，最终MobileOne-S2模型达到了77.4%的指标。  
- [EfficientFormer：苹果手机实时推理的Transformer模型，登顶轻量化Backbone之巅 | 集智书童](https://mp.weixin.qq.com/s/Ib6ckyjsDyafiQZKWBF8UQ)  
摘要：Vision Transformers由于大量的参数和模型设计，ViT常比轻量网络慢几倍。因此，应用部署尤其是移动端挑战大。通常大多通过网络架构搜索或与 MobileNet Block 的混合设计来降低计算复杂度，但推理速度仍然不能令人满意。这就引出了一个重要的问题：Transformer 能否在获得高性能的同时运行得像 MobileNet 一样快？  
首先重新审视基于 ViT 的模型中使用的网络架构和 ViT 算子，并确定其低效的设计。然后引入了一个维度一致的纯 Transformer （没有 MobileNet Block）作为设计范式。最后，执行延迟驱动的瘦身以获得一系列最终模型，称为 EfficientFormer。  
实验表明 EfficientFormer 在移动设备上的性能和速度方面具有优势。L1版本 在 ImageNet-1K 上实现了 79.2% 的 Top-1 准确率，在 iPhone 12（使用 CoreML 编译）上只有 1.6 ms 的推理延迟，甚至比 MobileNetV2（1.7 ms，71.8% Top-1)，EfficientFormer-L7 获得了 83.3% 的准确率，延迟仅为 7.0 ms。EfficientFormer证明，正确设计的 Transformer 可以在移动设备上达到极低的延迟，同时保持高性能。  




## 开源项目

- [武大与华为联合打造，全球首个遥感影像智能解译深度学习开源框架上线]()  
摘要：武汉大学与华为联合打造的武汉.LuoJia正式上线，武汉大学与华为基于昇腾 AI 共同打造了全球首个遥感影像智能解译专用框架武汉. LuoJiaNET 和业界最大遥感影像样本数据集武汉. LuoJiaSET。。通过提供遥感影像智能解译专用框架、高效且精准的样本标注工具、样本库以及相应的基础模型，武汉.LuoJia 为遥感应用开发提供便利，让智能遥感技术在自然资源、海洋、农业、森林、应急等行业得到广泛应用。  
- [飞桨框架v2.3发布，高复用性算子库、异构多云分布式训练等多项新特性重磅升级 | 飞桨PaddlePaddle](https://mp.weixin.qq.com/s/X7SjN_TWJsOHUormhdbt1A)  
摘要：该版本的重点更新：更加丰富的API体系、高复用性算子库PHI、高扩展性参数服务器、全流程硬件感知的性能自动调优、自动化压缩和推理引擎性能优化、大模型训练和推理能力全面升级、异构多云自适应分布式训练架构。
其中，算子库是深度学习框架极为重要的基础组成部分，也是硬件和框架适配的重要桥梁，但算子开发对很多开发者而言挑战比较大。飞桨框架2.3版本打造了高复用性的算子库PHI，通过函数式接口为高阶开发者提供了更简单的算子开发方式，有效降低算子开发门槛，提升开发效率，并且能够大幅降低芯片对飞桨框架的算子适配成本。下面具体看一下PHI算子库的三个特点：
函数式算子接口，可以通过函数调用的方式复用基础算子来进行更复杂的组合算子开发。例如，通过对矩阵乘、加减乘除法等基础算子函数式接口的调用，很容易实现一个FC或者SGD算子，高效支撑自定义算子开发需求。以非常复杂的Einsum算子为例，通过利用PHI算子库，这类复杂算子的开发有低成本的显著优势。
插件式算子管理机制，避免和框架代码耦合，支持低成本地复用硬件加速库的能力。
提供Primitive的算子内核开发接口，实现不同芯片之间的算子内核开发代码的复用，提升算子内核的开发效率。例如，飞桨在GPU和XPU上，已实现了大量基于Primitive接口的算子内核的复用，大幅降低了硬件算子适配的成本。


## 博文

- [Tenstorrent芯片架构浅谈 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/y-P1X-QeLjozC7jvEeAolA)  
摘要：近年市场上的AI芯片层出不穷，根源上还是算法与应用均处于高速迭代，计算硬件底座自然需要不断更新。其中芯片公司Tenstorrent的芯片架构别具一格，本文尝试一探究竟。  
Tenstorrent共设计出3款芯片，其中Jawbridge是一款小型测试芯片，Grayskull和Wormhole则是对外商用芯片，可覆盖训练和推理场景。Tenstorrent的芯片架构设计目标是解决模型在训练或推理时无法高效灵活扩展（scale out）的问题，提出2个核心技术点：
    1. 摒弃传统的核间共享式内存架构，采用Multicore Private Memory Model：芯片重点在硬件层面和软件层面分别加强了数据通信能力。在硬件层自研片上互联Network on Chip(NoC), NoC是2D双向环路结构；
    2. 动态执行：
        2.1 运行时数据压缩，以设计较小容量的private memory，在芯片内多核间及芯片间的数据通信量也随之降低，进而从整体上可以获得更高的性能功耗比。另外，Packet Manager还可以处理reshape/flatten等tensor形状变换操作，且该操作可以与Compute Engine并行执行，时间上可以overlap；
        2.2 条件执行：片上的逻辑控制单元和计算单元均可以高效运行，避免了CPU fallback的问题，基于稀疏门控专家混合模型，可通过门口网络实现只激活模型的部分结构，在增加模型容量和能力下不会成比例增加计算量；
        2.3 稀疏计算：支持常规的对权重进行稀疏化之外，还支持对激活值进行分块稀疏，进而降低计算量；
        2.4 动态混合精度：可以在运行时或AOT阶段设置每个算子的计算精度。  
Tenstorrent通过软硬协同设计方式，将数据并行和模型并行的部分功能实现下沉到硬件层，有效解决了横向扩展问题，这样就可以替代当前主流深度学习框架在分布式实现方面的大量编码工作，进而降低了深度学习框架的开发和使用门槛。同时，硬件的变化并没有降低软件栈的通用性，其软件栈支持PyTorch等主流框架。另一方面，芯片具有高度模块化，多个芯片可通过标准以太网端口连接在一起，进而扩展成大型AI网络。由于芯片内已集成NoC，因此这种扩展并不需要额外的交换机，因此扩展灵活度很高。  
- [MegEngine Inference 卷积优化之 Im2col 和 winograd 优化 | MegEngine Bot](https://zhuanlan.zhihu.com/p/532187602)  
摘要：卷积在推理时的优化有方式，本文主要介绍 Im2col+matmul 卷积以及 Winograd 卷积方式。  
im2col+matmul：和矩阵乘具有很多相似的特点，因此该方法使用 Im2col 的操作将卷积运算转化为矩阵运算，最后调用高性能的 Matmul 进行计算。该方法适应性强，支持各种卷积参数的优化，在通道数稍大的卷积中性能基本与 Matmul 持平，并可与其他优化方法形成互补。  
Winograd：按照 Winograd 算法的原理将卷积进行转变，达到减少卷积运算中乘法总量。其主要是通过将卷积中的乘法使用加法来替换，并把一部分替换出来的加法放到 weight 的提前处理中，从而达到加速卷积计算的目的。Winograd 算法的优化局限为在一些特定的常用卷积参数才支持。  
- [向外借力：Pluto助力MLIR编译器的多面体优化 | 壁仞科技研究院](https://mp.weixin.qq.com/s/n33DyOeTjA93HavZBZb94g)  
摘要：多面体编译是一项成熟的编译优化技术，演进了几十年，在传统的编译器中常作为一种优化工具使用，比如LLVM中使用的Polly，在GCC中使用的GRAPHITE。近些年来，多面体技术也引入到AI编译器中，进行循环优化及算子融合优化等。  
本文将关注在MLIR中以类插件的形式引入多面体优化技术，补充其多面体优化能力。多面体编译优化关注的是在确保程序执行正确的前提下重组多重循环的结构，实现性能的最优化。变形的目的是为了实现并行计算，达到更好的性能。  
多面体优化是一项成熟的技术，但也受限于对仿射变换的依赖，对无法进行仿射的循环的优化能力较弱，存在一定的局限性，因此无法在工业界得到广泛应用。同时，多面体优化技术理论相对复杂难懂，从事相关研究的人员较少，难以进行落地。尽管如此，多面体技术在解决特定的问题方面尤其独特的作用，比如在深度学习领域，对多算子融合和多层循环优化方面有着极大的帮助，可以将现有的多面体技术引入到AI编译器中，进行特定功能的优化。  
