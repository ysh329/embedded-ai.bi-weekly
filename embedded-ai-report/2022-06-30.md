---
layout: default
---

# 嵌入式AI简报 (2022-06-30)：  


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：【新闻】；【论文】；【开源】；【博文】。


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 台积电：2nm N2技术路线图亮相，计划2025年生产，承诺让芯片设计人员在同功率和晶体管数量下将性能提升 10% 至 15%，或在相同频率和复杂度下将功耗降低 25% 至 30%。广泛使用 EUV 光刻技术，并引入了 GAAFET（台积电称之为纳米片晶体管）以及背面供电；预计台南的生产中心再建四座价值 100 亿美元的工厂，用于制造 3 nm芯片，覆盖 Apple SoC、 A 系列芯片；
Imagination：GPU获得ADAS和HMI应用的ISO 26262功能安全认证；
- AMD：支持芯片定制，向第三方Chiplet打开大门。允许客户如早先的索尼，微软游戏主机，可以在紧凑的芯片封装中实现多个裸片（也称为chiplet或compute tiles ）。AMD 已经在使用tiles，但现在欢迎第三方制造加速器或汽车等其他芯片，以将其与 x86 CPU 和 GPU ，AI加速器一起包含在 其2D 或 3D 封装中；
三星：六月中旬副董事长会见了 ASML CEO 讨论了高端芯片设备合作疑抢购EUV光刻机；宣布基于 GAA （全环栅晶体管） 量产3nm 工艺，或将是世界上第一个迈入 GAA 结构的晶圆厂；
- 龙芯：龙芯中科登陆科创板，其下一代3A6000预计2023年发布，采用12nm并有大幅度架构升级，单核SPEC CPU 2006定点/浮点base分值（GCC）从26/28分提高到35/45分，内存双通道DDR4的Stream带宽(峰值51.2GBps)也将从25GBps提高到38GBps；
- RISC-V：架构改进，宣布2022年的首批四项规格和扩展的批准：高效跟踪（E-Trace）、主管二进制接口（SBI）、统一可扩展固件接口（UEFI）规格，及Zmmul纯乘法扩展有利于简单的FPGA软核；


> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [高通AI软件栈：推动OEM厂商和开发者的AI开发，已商用上市 | 高通中国](https://mp.weixin.qq.com/s/e10-dPe0OqZBNGpP8AD8Hg)  
摘要：该AI软件栈是面向OEM厂商和开发者的一套完整的AI解决方案，覆盖智能手机、汽车、XR、计算、物联网和云平台。  
AI软件栈支持包括TensorFlow、PyTorch和ONNX在内的不同AI框架与主流runtimes，以及开发者库与服务、系统软件、工具和编译器，AI软件栈产品组合还支持一系列工具套件，包括高通AI模型增效工具包（AIMET）、AI开发图形用户界面（GUI）、用于增强量化与优化的模型分析器以及神经网络架构搜索（NAS）。  
2021骁龙技术峰会上，高通和Google Cloud宣布将Google Cloud Vertex AI NAS集成至高通神经网络处理SDK，赋能OEM厂商和生态系统打造高效的边缘侧体验。其SDK仍然是OEM厂商和开发者在高通技术公司各类产品上运行神经网络的关键。更多见：https://www.qualcomm.com/products/technology/artificial-intelligence/ai-stack  
- [


## 论文  

- [meituan/YOLOv6: YOLOv6——精度与速度远超 YOLOv5 和 YOLOX 的新框架 | 美团技术团队](https://mp.weixin.qq.com/s/RrQCP4pTSwpTmSgvly9evg)  
代码：https://github.com/meituan/YOLOv6  
摘要：本文介绍了美团视觉智能部在目标检测框架方面的优化及实践经验，针对 YOLO 系列框架，在训练策略、主干网络、多尺度特征融合、检测头等方面进行了思考和优化，设计了新的检测框架-YOLOv6，初衷来自于解决工业应用落地时所遇到的实际问题：
    1. 更高效的 Backbone 和 Neck ：受到硬件感知神经网络设计思想的启发，基于 RepVGG style 设计了可重参数化（其结构在训练时有多分支拓扑，实际部署时可融合为单个 3x3 卷积）、更高效的骨干网络 EfficientRep Backbone 和 Rep-PAN Neck；
    2. 优化设计了更简洁有效的 Efficient Decoupled Head，维持精度同时，降低了一般解耦头带来的额外延时开销；
    3. 训练策略：采用Anchor-free 无锚范式，同时辅以 SimOTA 标签分配策略以及 SIoU 边界框回归损失来进一步提高检测精度。
在工业界常用的尺寸模型中：YOLOv6-nano 在 COCO 上精度可达 35.0% AP，在 T4 上推理速度可达 1242 FPS；YOLOv6-s 在 COCO 上精度可达 43.1% AP，在 T4 上推理速度可达 520 FPS。在部署方面，YOLOv6 支持 GPU（TensorRT）、CPU（OPENVINO）、ARM（MNN、TNN、NCNN）等不同平台的部署，极大地简化工程部署时的适配工作。  
- [MobileOne: 移动端仅需1ms的高性能骨干，你值得拥有 | AIWalker](https://mp.weixin.qq.com/s/crsRcY7dm6HJSd-QjcoUYg)  
摘要：MobileOne(≈MobileNetV1+RepVGG+训练Trick)是由Apple公司提出的一种基于iPhone12优化的超轻量型架构，在ImageNet数据集上以<1ms的速度取得了75.9%的Top1精度。  
为更好的分析高效率网络的瓶颈所在，作者以iPhone12平台为基准，从不同维度进行了"瓶颈"分析：发现参数多推理也可以跑得快，计算量大FLOPs也可以跑得快，那么换句话说，在移动端，延迟与FLOPs和参数量的相关性较弱，而在PC-CPU端，该相关性进一步弱化。  
作者做了进一步主要是性能，对激活函数、block结构分别选择ReLU和SE模块，选择RELU是因为其他激活函数慢，选择SE是因为其单分支结构也更快。基于上述分析，MobileOne的核心模块基于MobileNetV1而设计，同时吸收了重参数思想。



## 开源项目



## 博文

- [Tenstorrent芯片架构浅谈 | Adlik 深度学习推理工具链](https://mp.weixin.qq.com/s/y-P1X-QeLjozC7jvEeAolA)  
摘要：近年市场上的AI芯片层出不穷，根源上还是算法与应用均处于高速迭代，计算硬件底座自然需要不断更新。其中芯片公司Tenstorrent的芯片架构别具一格，本文尝试一探究竟。  
Tenstorrent共设计出3款芯片，其中Jawbridge是一款小型测试芯片，Grayskull和Wormhole则是对外商用芯片，可覆盖训练和推理场景。Tenstorrent的芯片架构设计目标是解决模型在训练或推理时无法高效灵活扩展（scale out）的问题，提出2个核心技术点：
    1. 摒弃传统的核间共享式内存架构，采用Multicore Private Memory Model：芯片重点在硬件层面和软件层面分别加强了数据通信能力。在硬件层自研片上互联Network on Chip(NoC), NoC是2D双向环路结构；
    2. 动态执行：
        2.1 运行时数据压缩，以设计较小容量的private memory，在芯片内多核间及芯片间的数据通信量也随之降低，进而从整体上可以获得更高的性能功耗比。另外，Packet Manager还可以处理reshape/flatten等tensor形状变换操作，且该操作可以与Compute Engine并行执行，时间上可以overlap；
        2.2 条件执行：片上的逻辑控制单元和计算单元均可以高效运行，避免了CPU fallback的问题，基于稀疏门控专家混合模型，可通过门口网络实现只激活模型的部分结构，在增加模型容量和能力下不会成比例增加计算量；
        2.3 稀疏计算：支持常规的对权重进行稀疏化之外，还支持对激活值进行分块稀疏，进而降低计算量；
        2.4 动态混合精度：可以在运行时或AOT阶段设置每个算子的计算精度。  
Tenstorrent通过软硬协同设计方式，将数据并行和模型并行的部分功能实现下沉到硬件层，有效解决了横向扩展问题，这样就可以替代当前主流深度学习框架在分布式实现方面的大量编码工作，进而降低了深度学习框架的开发和使用门槛。同时，硬件的变化并没有降低软件栈的通用性，其软件栈支持PyTorch等主流框架。另一方面，芯片具有高度模块化，多个芯片可通过标准以太网端口连接在一起，进而扩展成大型AI网络。由于芯片内已集成NoC，因此这种扩展并不需要额外的交换机，因此扩展灵活度很高。  
- [MegEngine Inference 卷积优化之 Im2col 和 winograd 优化 | MegEngine Bot](https://zhuanlan.zhihu.com/p/532187602)  
摘要：卷积在推理时的优化有方式，本文主要介绍 Im2col+matmul 卷积以及 Winograd 卷积方式。  
im2col+matmul：和矩阵乘具有很多相似的特点，因此该方法使用 Im2col 的操作将卷积运算转化为矩阵运算，最后调用高性能的 Matmul 进行计算。该方法适应性强，支持各种卷积参数的优化，在通道数稍大的卷积中性能基本与 Matmul 持平，并可与其他优化方法形成互补。  
Winograd：按照 Winograd 算法的原理将卷积进行转变，达到减少卷积运算中乘法总量。其主要是通过将卷积中的乘法使用加法来替换，并把一部分替换出来的加法放到 weight 的提前处理中，从而达到加速卷积计算的目的。Winograd 算法的优化局限为在一些特定的常用卷积参数才支持。
        
    
