---
layout: default
---

# 嵌入式AI简报 (2022-02-22)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- Arm：英伟达收购 Arm 交易失败，日本软银迅速宣布 Arm 将 IPO ，但中国的合资公司安谋科技（Arm中国）的存在，让目前 IPO 看来很不稳定，因自2020年以来Arm一直在与其中国合资公司进行法律斗争要解雇当时的合资企业领导人，但以失败告终。Arm对现状并不满意，因为该公司大约 20% 的财务无法被审计。Arm 中国发言人称“作为其在中国的长期合作伙伴和价值贡献者，完全支持 Arm 的 IPO 计划”；
- 英特尔：①2月17日，CEO基尔辛格表示：“我们不是ARM的大客户，但也确实在用ARM的技术。随着我们将ARM纳入我们的代工业务，我们将成为ARM的更大客户。若有财团收购ARM，我们非常愿意参与进来”；②将开放 x86 架构的软核和硬核授权，使客户能够在英特尔制造的定制设计芯片中混合 x86、Arm 和 RISC-V 等不同的 CPU IP 核；③2月15日，拟60亿美元收购高端模拟芯片代工厂以色列高塔半导体公司，加深英特尔在全球最大的代工芯片制造商台积电主导的领域中的地位；④英宣布加入RISC-V International，投资10亿美元全力支持RISC-V，旨在利用Intel最新的创新芯片架构和先进的封装技术，加快客户产品进入市场的时间；
- 智能手机：Counterpoint Research 2021Q4市场监测报显示，2021Q4中国智能手机出货量同比下降 11% 。苹果高居首位，达到历史最高市场份额，OPPO加上子品牌 OnePlus 升至第三。vivo作为第四，以16.5%的市场份额位列第四。小米跌至第五位，延续跌势；
- 联发科：发布Pentonic 2000 旗舰智能电视芯片（台积电 7nm），强劲 CPU+GPU+APU，APU拥有多项先进 AI 能力如超级分辨率，支持 8K 120Hz 超清显示；
- 华为麒麟：麒麟 9000L 将首发于即将到来的 2022 款华为 Mate 40E（Pro） 5G，kirin9000L设计类似于9000E和9000，但似乎是由三星 5nm EUV 工艺代工，相比台积电代工的两款前辈型号略有不及；
- 三星：4nm旗舰芯Exynos 2200，“1+3+4”设计，超大核为Cortex X2，多核成绩超骁龙8，Galaxy S22首发；
- 高通：因三星代工无法解决 4nm 骁龙8 Gen1 Plus地发热问题，高通转而向最新处理器要找台积电代工，目前Snapdragon8 Gen 1 Plus推测为Cortex-X2的CPU核心，Adreno 730 单个Kryo GPU 核心；
- 紫光展锐：董事长变更，赵伟国退出，吴胜武接任，后者自2019年担任清华紫光全球执行副总裁兼厦门统一集团有限公司董事长，也现任全国青联委员、浙江大学电子服务研究院任客座研究员、清华大学兼职导师等职；
- 台积电：为吸引及留任公司高管及人才，拿出700亿元新台币为员工分红、发奖金；日本工厂提前开始招人：包括应届生，国籍不限，虽然硕士或博士毕业生。另外，日语N2以上的资格要求，但与国内半导体人才待遇相比，并不算高；
- ASML：ASML在最新年度报告中指出，一家与之前因窃密而判赔的XTAL 相关的中国公司，可能销售侵害其知识产权的产品。ASML认为这家公司是东方晶源微电子，ASML 已要求特定客户避免与XTAL 中国母公司东方晶源微电子业务往来，同时也密切关注事态发展，准备在适当时机采取法律行动。




> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [Google TensorFlow Lite 技术主管皮特·沃登离职，重返斯坦福读博：我在谷歌“太难了” | AI科技评论](https://mp.weixin.qq.com/s/qmb7ZkeszX1NRK1XZg01uA)  
摘要：据Pete Warden（皮特沃登）本人推特消息，他将离开谷歌公司，重返斯坦福大学攻读计算机博士学位。  
皮特沃登是谷歌公司Tensorflow面向移动和嵌入式设备部分的技术主管，也是 Tensorflow团队的创始成员之一。著有《TinyML》一书，希望让机器学习不再囿于云端超级计算机，而是可以被隐藏于众多小到可以被忽视的电子零件中。  
离开谷歌的原因，皮特沃登说：“it’s very costly and time-consuming to launch new hardware devices at Google, because the downsides of a failed or buggy launch to any large company’s reputation are so high. ”  
- [Intel 4下半年可投产！英特尔公布技术路线图及重要节点 | EETOP](https://mp.weixin.qq.com/s/6SNUsc68WJLNb-2QQ9R4ZQ)  
摘要：英特尔2022年投资者大会上，英特尔CEO帕特·基辛格和各业务部门负责人概述了公司发展战略及长期增长规划，主要包括：数据中心与人工智能、客户端计算、加速计算系统与图形、英特尔代工服务等。  
    - 数据中心与人工智能：从2022年第一季度开始，英特尔将交付采用Intel 7制程工艺制造的Sapphire Rapids处理器，仅在AI方面即可实现高达30倍的性能提升；未来几代至强将同时拥有基于性能核（P-core）和能效核（E-core）的双轨产品路线图；2024年将推出基于3nm地能效核至强处理器Sierra Forest；
    - 加速计算系统与图形：预计将在2022年出货超400万颗的独立GPU；让至强®处理器实现更高算力与内存带宽，并交付具有行业领导力的CPU和GPU产品路线图；
    - 英特尔代工服务（IFS）：正组建一个为汽车制造商提供完整的解决方案地团队，重点关注开放的中央计算架构、汽车级代工平台、向先进技术的过渡。


## 论文  





## 开源项目

- [arogozhnikov/einops：斯拉AI高管都推荐的张量工具，开源了三年后终于中顶会ICLR 2022 Oral | 量子位](https://mp.weixin.qq.com/s/QxowSMirwnsUjIA-MFCj7g)  
代码：https://github.com/arogozhnikov/einops  
摘要：Flexible and powerful tensor operations for readable and reliable code. Supports numpy, pytorch, tensorflow, jax, and others.  
该框架基于爱因斯坦求和约定（Einstein summation convention）的思路开发，能够大幅提高代码的可读性和易修改性。同时，Einops支持Pytorch、TensorFlow、Chainer、Jax、Gluon等多个深度学习框架，以及Numpy、Cupy等张量计算框架。ICLR 2022将其接收为Oral论文。  
Einops的基本原理来自于爱因斯坦在1916年提出的爱因斯坦求和约定，也叫爱因斯坦标记法（Einstein notation）：当一组乘积中，有两个变量的脚标一样，就要对相同的两个脚标求和，可避免公式里出现大量的求和符号，看起来更简洁如Numpy种就有使用。  
但Einops正是基于Einsum进行了诸多改进，针对张量操作过程中一些以前难以解决的问题，提供了更加便利的方案。那不得不说Einops的本质：通过针对变换模式的新的标记法，能够确保元素在张量中的位置与坐标变量的值一对一映射。如`nn.transpose(x, [0,3,1,2])`可以写成`rearrange(x, 'b h w c -> b c h w')`。与爱因斯坦求和约定（Einsum）相比，Einops有3个规则，让Einops的代码可读性很高：
    1. axis present only in the input (the left hand side) is reduced (e.g. with max-reduction)
    2. axis present only in the output is “repeated” (tensor values are the same for all index values of
new axes)
    3. all axis identifiers on either side of expression should be unique (numpy.einsum allows repeats to
cover traces).
综上，einops可灵活地处理高维度数据。

## 博文



- [MegEngine 的 CUDA 矩阵乘法终极优化 | 旷视研究院](https://mp.weixin.qq.com/s/XX5q36gwfqKyPaQOkiUx8w)  
摘要：单精度矩阵乘法（SGEMM）几乎是每一位学习 CUDA 的同学绕不开的案例，这个经典的计算密集型案例可以很好地展示 GPU 编程中常用的优化技巧，而能否写出高效率的 SGEMM Kernel ，也是反应每一位 CUDA 程序员对 GPU 体系结构的理解程度的优秀考题。本文将详细介绍 CUDA SGEMM 的优化手段，适合认真阅读过《CUDA C++Programming Guide》，具备一定 CUDA 编程基础的同学阅读，希望能给追求极致性能的同学们一些启发。  
- [快手特效业务的云端渲染方案 | 快手Y-tech团队](https://mp.weixin.qq.com/s/MG3NUy4R4pDbsZhp0i6BsA)  
摘要：云渲染就是依托于云计算的一种云端渲染服务。用户将本地的渲染任务提交到远程服务器，通过远程的计算机集群资源进行运算操作，将上传的任务进行云端处理后再返回给本地，由用户下载提取。使用云端渲染可以更好的支持业务需求，如随时支持特效相关的H5活动，以及充分利用服务端的大内存和高性能，结合Y-tech的AI能力，给用户提供更完美好看、好玩的画面效果。  
主要的难点之一便是云端渲染的耗时优化，随着输入视频时长的增加以及业务场景复杂度的提升，耗时的增加导致用户的等待时长以及服务器资源的成本都在不断提高，所以云端渲染急需在性能耗时等方面做各种优化处理。  
通过充分利用服务器的多核优势：将解码、渲染、编码三个模块解耦拆分、进行多线程异步处理。针对一些长视频处理过程中的瓶颈，进一步将编码、解码模块内部重构，支持多线程解码、多线程编码，在性能上满足更复杂的需求。使用一分钟的输入视频进行测试，特效及输出不变的情况下，针对多线程方面的优化耗时提升明显，优化后CPU和GPU利用率是之前的三倍，耗时较之前可提高2.5倍左右。  
- [google/jax：2022年，我该用JAX吗？GitHub 1.6万星，这个年轻的工具并不完美 | 机器之心](https://mp.weixin.qq.com/s/5_0QP7NxPI44fG1uv6e40Q)  
摘要：近年来，谷歌于 2018 年推出的 JAX 迎来了迅猛发展，很多研究者对其寄予厚望，希望它可以取代 TensorFlow 等众多深度学习框架。但 JAX 是否真的适合所有人使用呢？这篇文章对 JAX 的方方面面展开了深入探讨，希望可以给研究者选择深度学习框架时提供有益的参考。  
JAX 不是一个深度学习框架或库，其设计初衷也不是成为一个深度学习框架或库。简而言之，JAX 是一个包含可组合函数转换的数值计算库。JAX 的定位科学计算（Scientific Computing）和函数转换（Function Transformations）的交叉融合，具有除训练深度学习模型以外的一系列能力，包括如下：  
    1. 即时编译（Just-in-Time Compilation）：JAX 允许用户使用 XLA 将自己的函数转换为即时编译（JIT）版本。这意味着可以通过在计算函数中添加一个简单的函数装饰器（decorator）来将计算速度提高几个数量级；
    2. 自动并行化（Automatic Parallelization）
    3. 自动向量化（Automatic Vectorization）
    4. 自动微分（Automatic Differentiation）  
总之，虽然JAX是实验性项目，但使用它的根本原因是速度且不依赖传统仿真模拟软件，与之相关的底层项目XLA（Accelerated Linear Algebra）是专为线性代数设计的全程序优化编译器，而 JAX 建立在 XLA 之上，进一步提高了计算速度上限。  









