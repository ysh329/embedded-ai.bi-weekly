---
layout: default
---

# 嵌入式AI简报 (2022-02-22)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 英特尔：2月17日，CEO基尔辛格表示：“我们不是ARM的大客户，但我们确实在使用ARM的技术。随着我们也将ARM纳入我们的IFS(代工业务)议程，我们将成为ARM的更大客户。如果有财团来收购ARM，我们可能非常愿意以某种方式参与进来”；
- 智能手机：Counterpoint Research 2021Q4市场监测报显示，2021Q4中国智能手机出货量同比下降 11% 。苹果高居首位，达到历史最高市场份额，OPPO加上子品牌 OnePlus 升至第三。vivo作为第四，以16.5%的市场份额位列第四。小米跌至第五位，延续跌势；




> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [Google TensorFlow Lite 技术主管皮特·沃登离职，重返斯坦福读博：我在谷歌“太难了” | AI科技评论](https://mp.weixin.qq.com/s/qmb7ZkeszX1NRK1XZg01uA)  
摘要：据Pete Warden（皮特沃登）本人推特消息，他将离开谷歌公司，重返斯坦福大学攻读计算机博士学位。  
皮特沃登是谷歌公司Tensorflow面向移动和嵌入式设备部分的技术主管，也是 Tensorflow团队的创始成员之一。著有《TinyML》一书，希望让机器学习不再囿于云端超级计算机，而是可以被隐藏于众多小到可以被忽视的电子零件中。  
离开谷歌的原因，皮特沃登说：“it’s very costly and time-consuming to launch new hardware devices at Google, because the downsides of a failed or buggy launch to any large company’s reputation are so high. ”  



## 论文  





## 开源项目

- [arogozhnikov/einops：斯拉AI高管都推荐的张量工具，开源了三年后终于中顶会ICLR 2022 Oral | 量子位](https://mp.weixin.qq.com/s/QxowSMirwnsUjIA-MFCj7g)  
代码：https://github.com/arogozhnikov/einops  
摘要：Flexible and powerful tensor operations for readable and reliable code. Supports numpy, pytorch, tensorflow, jax, and others.  
该框架基于爱因斯坦求和约定（Einstein summation convention）的思路开发，能够大幅提高代码的可读性和易修改性。同时，Einops支持Pytorch、TensorFlow、Chainer、Jax、Gluon等多个深度学习框架，以及Numpy、Cupy等张量计算框架。ICLR 2022将其接收为Oral论文。  
Einops的基本原理来自于爱因斯坦在1916年提出的爱因斯坦求和约定，也叫爱因斯坦标记法（Einstein notation）：当一组乘积中，有两个变量的脚标一样，就要对相同的两个脚标求和，可避免公式里出现大量的求和符号，看起来更简洁如Numpy种就有使用。  
但Einops正是基于Einsum进行了诸多改进，针对张量操作过程中一些以前难以解决的问题，提供了更加便利的方案。那不得不说Einops的本质：通过针对变换模式的新的标记法，能够确保元素在张量中的位置与坐标变量的值一对一映射。如`nn.transpose(x, [0,3,1,2])`可以写成`rearrange(x, 'b h w c -> b c h w')`。与爱因斯坦求和约定（Einsum）相比，Einops有3个规则，让Einops的代码可读性很高：
    1. axis present only in the input (the left hand side) is reduced (e.g. with max-reduction)
    2. axis present only in the output is “repeated” (tensor values are the same for all index values of
new axes)
    3. all axis identifiers on either side of expression should be unique (numpy.einsum allows repeats to
cover traces).
综上，einops可灵活地处理高维度数据。

## 博文



- [MegEngine 的 CUDA 矩阵乘法终极优化 | 旷视研究院](https://mp.weixin.qq.com/s/XX5q36gwfqKyPaQOkiUx8w)  
摘要：单精度矩阵乘法（SGEMM）几乎是每一位学习 CUDA 的同学绕不开的案例，这个经典的计算密集型案例可以很好地展示 GPU 编程中常用的优化技巧，而能否写出高效率的 SGEMM Kernel ，也是反应每一位 CUDA 程序员对 GPU 体系结构的理解程度的优秀考题。本文将详细介绍 CUDA SGEMM 的优化手段，适合认真阅读过《CUDA C++Programming Guide》，具备一定 CUDA 编程基础的同学阅读，希望能给追求极致性能的同学们一些启发。  
- [快手特效业务的云端渲染方案 | 快手Y-tech团队](https://mp.weixin.qq.com/s/MG3NUy4R4pDbsZhp0i6BsA)  
摘要：云渲染就是依托于云计算的一种云端渲染服务。用户将本地的渲染任务提交到远程服务器，通过远程的计算机集群资源进行运算操作，将上传的任务进行云端处理后再返回给本地，由用户下载提取。使用云端渲染可以更好的支持业务需求，如随时支持特效相关的H5活动，以及充分利用服务端的大内存和高性能，结合Y-tech的AI能力，给用户提供更完美好看、好玩的画面效果。  
主要的难点之一便是云端渲染的耗时优化，随着输入视频时长的增加以及业务场景复杂度的提升，耗时的增加导致用户的等待时长以及服务器资源的成本都在不断提高，所以云端渲染急需在性能耗时等方面做各种优化处理。  
通过充分利用服务器的多核优势：将解码、渲染、编码三个模块解耦拆分、进行多线程异步处理。针对一些长视频处理过程中的瓶颈，进一步将编码、解码模块内部重构，支持多线程解码、多线程编码，在性能上满足更复杂的需求。使用一分钟的输入视频进行测试，特效及输出不变的情况下，针对多线程方面的优化耗时提升明显，优化后CPU和GPU利用率是之前的三倍，耗时较之前可提高2.5倍左右。  
- [google/jax：2022年，我该用JAX吗？GitHub 1.6万星，这个年轻的工具并不完美 | 机器之心](https://mp.weixin.qq.com/s/5_0QP7NxPI44fG1uv6e40Q)  
摘要：近年来，谷歌于 2018 年推出的 JAX 迎来了迅猛发展，很多研究者对其寄予厚望，希望它可以取代 TensorFlow 等众多深度学习框架。但 JAX 是否真的适合所有人使用呢？这篇文章对 JAX 的方方面面展开了深入探讨，希望可以给研究者选择深度学习框架时提供有益的参考。  
JAX 不是一个深度学习框架或库，其设计初衷也不是成为一个深度学习框架或库。简而言之，JAX 是一个包含可组合函数转换的数值计算库。JAX 的定位科学计算（Scientific Computing）和函数转换（Function Transformations）的交叉融合，具有除训练深度学习模型以外的一系列能力，包括如下：  
    1. 即时编译（Just-in-Time Compilation）：JAX 允许用户使用 XLA 将自己的函数转换为即时编译（JIT）版本。这意味着可以通过在计算函数中添加一个简单的函数装饰器（decorator）来将计算速度提高几个数量级；
    2. 自动并行化（Automatic Parallelization）
    3. 自动向量化（Automatic Vectorization）
    4. 自动微分（Automatic Differentiation）  
总之，虽然JAX是实验性项目，但使用它的根本原因是速度且不依赖传统仿真模拟软件，与之相关的底层项目XLA（Accelerated Linear Algebra）是专为线性代数设计的全程序优化编译器，而 JAX 建立在 XLA 之上，进一步提高了计算速度上限。  









