---
layout: default
---

# 嵌入式AI简报 (2022-02-22)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 欧盟：继美国之后，欧盟也在芯片上正式公布《欧盟芯片法案》，计划砸下3000亿元巨额投资和补贴，力争实现2030年市场份额翻一番，达到全球20%。
- Arm：英伟达收购 Arm 交易失败，日本软银迅速宣布 Arm 将 IPO ，但中国的合资公司安谋科技（Arm中国）的存在，让目前 IPO 看来很不稳定，因自2020年以来Arm一直在与其中国合资公司进行法律斗争要解雇当时的合资企业领导人，但以失败告终。Arm对现状并不满意，因为该公司大约 20% 的财务无法被审计。Arm 中国发言人称“作为其在中国的长期合作伙伴和价值贡献者，完全支持 Arm 的 IPO 计划”；
- 英特尔：①2月17日，CEO基尔辛格表示：“我们不是ARM的大客户，但也确实在用ARM的技术。随着我们将ARM纳入我们的代工业务，我们将成为ARM的更大客户。若有财团收购ARM，我们非常愿意参与进来”；②将开放 x86 架构的软核和硬核授权，使客户能够在英特尔制造的定制设计芯片中混合 x86、Arm 和 RISC-V 等不同的 CPU IP 核；③2月15日，拟60亿美元收购高端模拟芯片代工厂以色列高塔半导体公司，加深英特尔在全球最大的代工芯片制造商台积电主导的领域中的地位；④英宣布加入RISC-V International，投资10亿美元全力支持RISC-V，旨在利用Intel最新的创新芯片架构和先进的封装技术，加快客户产品进入市场的时间；
- 智能手机：Counterpoint Research 2021Q4市场监测报显示，2021Q4中国智能手机出货量同比下降 11% 。苹果高居首位，达到历史最高市场份额，OPPO加上子品牌 OnePlus 升至第三。vivo作为第四，以16.5%的市场份额位列第四。小米跌至第五位，延续跌势；
- 联发科：发布Pentonic 2000 旗舰智能电视芯片（台积电 7nm），强劲 CPU+GPU+APU，APU拥有多项先进 AI 能力如超级分辨率，支持 8K 120Hz 超清显示；
- 华为麒麟：麒麟 9000L 将首发于即将到来的 2022 款华为 Mate 40E（Pro） 5G，kirin9000L设计类似于9000E和9000，但似乎是由三星 5nm EUV 工艺代工，相比台积电代工的两款前辈型号略有不及；
- 三星：4nm旗舰芯Exynos 2200，“1+3+4”设计，超大核为Cortex X2，多核成绩超骁龙8，Galaxy S22首发；
- 高通：因三星代工无法解决 4nm 骁龙8 Gen1 Plus地发热问题，高通转而向最新处理器要找台积电代工，目前Snapdragon8 Gen 1 Plus推测为Cortex-X2的CPU核心，Adreno 730 单个Kryo GPU 核心；
- 紫光展锐：董事长变更，赵伟国退出，吴胜武接任，后者自2019年担任清华紫光全球执行副总裁兼厦门统一集团有限公司董事长，也现任全国青联委员、浙江大学电子服务研究院任客座研究员、清华大学兼职导师等职；
- 台积电：①为吸引及留任公司高管及人才，拿出700亿元新台币为员工分红、发奖金；②日本工厂提前开始招人：包括应届生，国籍不限，虽然硕士或博士毕业生，但与国内半导体人才待遇相比，并不算高；③赴美设厂的计划，可能正陷入延宕，已延半年，推测原因应是美国反反覆覆的疫情，再加上人力不足等因素影响；
- ASML：最新年度报告中指出，一家与之前因窃密而判赔的XTAL相关的中国公司，可能销售侵害其知识产权的产品。ASML认为侵害公司是东方晶源微电子，ASML已要求特定客户避免与中国母公司东方晶源微电子业务往来，准备在适当时机采取法律行动。




> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  


- [Google TensorFlow Lite 技术主管皮特·沃登离职，重返斯坦福读博：我在谷歌“太难了” | AI科技评论](https://mp.weixin.qq.com/s/qmb7ZkeszX1NRK1XZg01uA)  
摘要：据Pete Warden（皮特沃登）本人推特消息，他将离开谷歌公司，重返斯坦福大学攻读计算机博士学位。  
皮特沃登是谷歌公司Tensorflow面向移动和嵌入式设备部分的技术主管，也是 Tensorflow团队的创始成员之一。著有《TinyML》一书，希望让机器学习不再囿于云端超级计算机，而是可以被隐藏于众多小到可以被忽视的电子零件中。  
离开谷歌的原因，皮特沃登说：“it’s very costly and time-consuming to launch new hardware devices at Google, because the downsides of a failed or buggy launch to any large company’s reputation are so high. ”  
- [Intel 4下半年可投产！英特尔公布技术路线图及重要节点 | EETOP](https://mp.weixin.qq.com/s/6SNUsc68WJLNb-2QQ9R4ZQ)  
摘要：英特尔2022年投资者大会上，英特尔CEO帕特·基辛格和各业务部门负责人概述了公司发展战略及长期增长规划，主要包括：数据中心与人工智能、客户端计算、加速计算系统与图形、英特尔代工服务等。  
    - 数据中心与人工智能：从2022年第一季度开始，英特尔将交付采用Intel 7制程工艺制造的Sapphire Rapids处理器，仅在AI方面即可实现高达30倍的性能提升；未来几代至强将同时拥有基于性能核（P-core）和能效核（E-core）的双轨产品路线图；2024年将推出基于3nm地能效核至强处理器Sierra Forest；
    - 加速计算系统与图形：预计将在2022年出货超400万颗的独立GPU；让至强®处理器实现更高算力与内存带宽，并交付具有行业领导力的CPU和GPU产品路线图；
    - 英特尔代工服务（IFS）：正组建一个为汽车制造商提供完整的解决方案地团队，重点关注开放的中央计算架构、汽车级代工平台、向先进技术的过渡。
- [国家级「东数西算」工程全面解读：8个算力枢纽，10大数字中心集群！ | 新智元](https://mp.weixin.qq.com/s/DjVWQKH8VWm3gBKiBUDF0Q)  
摘要：近日，国家发展改革委、中央网信办、工业和信息化部、国家能源局联合印发通知，同意在京津冀、长三角、粤港澳大湾区、成渝、内蒙古、贵州、甘肃、宁夏等8地启动建设国家算力枢纽节点，并规划了10个国家数据中心集群。「东数西算」就是通过构建数据中心、云计算、大数据一体化的新型算力网络体系，将东部算力需求有序引导到西部，优化数据中心建设布局，促进东西部协同联动。简单地说，就是让西部的算力资源更充分地支撑东部数据的运算，更好为数字化发展赋能。  


## 论文  


- [ICLR2022] [BiBERT: 面向极限压缩的自然语言任务的全二值化模型 | 量子位](https://mp.weixin.qq.com/s/gQJb5YhrNYgA-JK3AGeUdQ)  
文章：https://openreview.net/forum?id=5xEgrl_5FAJ  
代码：
摘要：现有的模型压缩方法包括参数量化、蒸馏、剪枝、参数共享等等。其中，参数量化方法高效地通过将浮点参数转换为定点数表示，使模型变得紧凑。研究者们提出了许多方案例如Q-BERT、Q8BERT、GOBO等，但量化模型仍旧面临严重的表达能力有限和优化困难的问题。
本文将介绍由北航刘祥龙教授团队等带来的，首个用于自然语言任务的全二值量化BERT模型——BiBERT，将权重、激活和嵌入均量化到1比特（而不仅仅是将权重量化到1比特，而激活维持在4比特或更高）。使模型在推理时使用逐位运算操作，加快部署时硬件的推理速度。主要工作基于二值化性能下降的原因，提出两点改进：
1. 二值注意力（Bi-Attention）：解决前向传播中二值化后的注意力机制的信息退化问题（Softmax的结果二值化导致信息丧失退化为0），考虑到softmax结果有序且遵循概率分布，为缓解该退化问题，在softmax的结果上加偏移量然后再做bool等，以最大化熵过滤关键元素，乘法采用Bitwise乘法；
2. 方向匹配蒸馏（Direction-Matching Distillation）：解决后向传播中蒸馏的优化方向不匹配问题（注意力权重是两个二值化的激活直接相乘而得，所以处于决策边缘的值很容易被二值化到相反一侧，从而直接优化注意力权重常常在训练过程中发生优化方向失配问题），作者设计了新的蒸馏方案，即针对上游的Query、Key和Value矩阵，构建相似性矩阵进行对激活的蒸馏。
基于上面两个方法，BiBERT性能超过现有的BERT模型二值化方法，甚至优于采用更多比特的量化方案，理论上BiBERT能够带来56.3倍的FLOPs减少和31.2倍的模型存储节省。  
- [GNN如何加速？中科院计算所最新《图神经网络加速算法研究》综述论文阐述GNN加速算法体系 | 专知](https://mp.weixin.qq.com/s/_CDXjKdTxkzvLrWyMF4MOQ)  
摘要：本文对GNN算法的加速方法进行了全面的综述，通过图级和模型级的优化，有利于不同平台上与图相关的任务。尽管近年来GNN加速方法取得了巨大的成功和飞跃，但在这一研究领域仍存在许多有待解决的问题，主要体现在小号三个方向：
    1. 动态图的加速: 大多数加速方法采用静态图进行研究。然而，动态图在拓扑和特征空间方面比静态图更灵活，因此很难将这些方法直接应用于动态图。像KD-GCN[Yang等人，2020]和二值化DGCNN [Bahri等人，2021]这样的压缩方法利用一个特殊设计的模块将其扩展到动态图形，提供了一个动态图形加速的范例；
    2.硬件友好算法: 通过利用硬件特性，硬件友好算法有利于模型(或算法)在通用平台上的执行。最近的文献[Liu et al.， 2021c]的目标是弥补图采样算法和硬件特性之间的差距，利用位置感知优化来产生显著的图采样速度。然而，这提出了一个问题，在设计GNN加速的硬件友好算法时，应该仔细考虑哪些特征。
    3. 算法和硬件协同设计: 不同于GNN的特定领域硬件加速器，如HyGCN [Yan等人，2020b]直接为GNN定制数据路径，算法和硬件协同设计探索了算法和硬件感知的设计空间。具体来说，在GNN加速的这种前景下，总体上可以通过算法和硬件的同时设计来实现优化的协同效应。然而，据我们所知，目前在这方面的工作还很少。


## 开源项目


- [arogozhnikov/einops：斯拉AI高管都推荐的张量工具，开源了三年后终于中顶会ICLR 2022 Oral | 量子位](https://mp.weixin.qq.com/s/QxowSMirwnsUjIA-MFCj7g)  
代码：https://github.com/arogozhnikov/einops  
摘要：Flexible and powerful tensor operations for readable and reliable code. Supports numpy, pytorch, tensorflow, jax, and others.  
该框架基于爱因斯坦求和约定（Einstein summation convention）的思路开发，能够大幅提高代码的可读性和易修改性。同时，Einops支持Pytorch、TensorFlow、Chainer、Jax、Gluon等多个深度学习框架，以及Numpy、Cupy等张量计算框架。ICLR 2022将其接收为Oral论文。  
Einops的基本原理来自于爱因斯坦在1916年提出的爱因斯坦求和约定，也叫爱因斯坦标记法（Einstein notation）：当一组乘积中，有两个变量的脚标一样，就要对相同的两个脚标求和，可避免公式里出现大量的求和符号，看起来更简洁如Numpy种就有使用。  
但Einops正是基于Einsum进行了诸多改进，针对张量操作过程中一些以前难以解决的问题，提供了更加便利的方案。那不得不说Einops的本质：通过针对变换模式的新的标记法，能够确保元素在张量中的位置与坐标变量的值一对一映射。如`nn.transpose(x, [0,3,1,2])`可以写成`rearrange(x, 'b h w c -> b c h w')`。与爱因斯坦求和约定（Einsum）相比，Einops有3个规则，让Einops的代码可读性很高：
    1. axis present only in the input (the left hand side) is reduced (e.g. with max-reduction)
    2. axis present only in the output is “repeated” (tensor values are the same for all index values of
new axes)
    3. all axis identifiers on either side of expression should be unique (numpy.einsum allows repeats to
cover traces).
综上，einops可灵活地处理高维度数据。  
- [Tencent/NCNN: PNNX —— PyTorch Neural Network Exchange，逃离 ONNX | 知乎](https://zhuanlan.zhihu.com/p/427620428)  
链接：https://zhuanlan.zhihu.com/p/427620428  
代码：https://github.com/Tencent/ncnn/pull/3262  
摘要：虽是去年的，但我觉得可以学习。 ncnn 的作者 nihui 提出了 PNNX 模型格式，其进一步强化了 PyTorch 训练 + ncnn 部署的社区生态。PNNX 作为 PyTorch 模型部署的新的方式，可以避开 ONNX 中间商，导出比较干净的高层 OP， 也代表着 PNNX 希望能比 ONNX 做的更好。
PNNX 没有发明新的算子标准，直接利用 PyTorch API ，在 ncnn 的易于修改的模型文件上，与 PNNX 格式映射，非常易用，不采用 MLIR 是因为 Google 主导且不稳定，而 PyTorch 和 TF 是两个生态，同时支持并不好玩儿。
PNNX 设计上说，支持静态 shape，也支持动态 shape ，其实现是从 torchscript 再捏回到原先高层 op，所以还是需要做 torchscript ir 的图优化， PNNX 写了这个叫 GraphRewritter 的类，只要指定我们的 pattern 的 ir，指定好后，就会自动从 torchscript 里找到一个匹配的封闭子图，然后把它替换成我们的 target 的 op，最后这个 target op 的一些参数也可以，从匹配的子图里去获取到原先那个参数，然后写到 target 参数里，PNNX 代码里面就有大量的捏回原始高层算子的这个过程，所以有一个专门做的基础设施，方便做捏算子的图优化过程。
目前 PNNX 兼容 PyTorch 1.8、1.9 和 1.10 版本，也可以导出 159 种 PyTorch 上层 op，其中有 80 个可以导出成 ncnn 对应的 op，也做了自动单元测试，还有 79% 的代码覆盖率。目前常用的 CNN 模型，像 resnet，shufflenet 这种模型，都是可以完美工作，就是 torchscript，转 PNNX、转 python 或 ncnn 这些都可以正常搞定，然后出来的推理结果跟原始的 python 是一模一样的。  
- [google/jax：2022年，我该用JAX吗？GitHub 1.6万星，这个年轻的工具并不完美 | 机器之心](https://mp.weixin.qq.com/s/5_0QP7NxPI44fG1uv6e40Q)  
摘要：近年来，谷歌于 2018 年推出的 JAX 迎来了迅猛发展，很多研究者对其寄予厚望，希望它可以取代 TensorFlow 等众多深度学习框架。但 JAX 是否真的适合所有人使用呢？这篇文章对 JAX 的方方面面展开了深入探讨，希望可以给研究者选择深度学习框架时提供有益的参考。  
JAX 不是一个深度学习框架或库，其设计初衷也不是成为一个深度学习框架或库。简而言之，JAX 是一个包含可组合函数转换的数值计算库。JAX 的定位科学计算（Scientific Computing）和函数转换（Function Transformations）的交叉融合，具有除训练深度学习模型以外的一系列能力，包括如下：  
    1. 即时编译（Just-in-Time Compilation）：JAX 允许用户使用 XLA 将自己的函数转换为即时编译（JIT）版本。这意味着可以通过在计算函数中添加一个简单的函数装饰器（decorator）来将计算速度提高几个数量级；
    2. 自动并行化（Automatic Parallelization）
    3. 自动向量化（Automatic Vectorization）
    4. 自动微分（Automatic Differentiation）  
总之，虽然JAX是实验性项目，但使用它的根本原因是速度且不依赖传统仿真模拟软件，与之相关的底层项目XLA（Accelerated Linear Algebra）是专为线性代数设计的全程序优化编译器，而 JAX 建立在 XLA 之上，进一步提高了计算速度上限。  


## 博文


- [MegEngine 的 CUDA 矩阵乘法终极优化 | 旷视研究院](https://mp.weixin.qq.com/s/XX5q36gwfqKyPaQOkiUx8w)  
摘要：单精度矩阵乘法（SGEMM）几乎是每一位学习 CUDA 的同学绕不开的案例，这个经典的计算密集型案例可以很好地展示 GPU 编程中常用的优化技巧，而能否写出高效率的 SGEMM Kernel ，也是反应每一位 CUDA 程序员对 GPU 体系结构的理解程度的优秀考题。本文将详细介绍 CUDA SGEMM 的优化手段，适合认真阅读过《CUDA C++Programming Guide》，具备一定 CUDA 编程基础的同学阅读，希望能给追求极致性能的同学们一些启发。  
- [快手特效业务的云端渲染方案 | 快手Y-tech团队](https://mp.weixin.qq.com/s/MG3NUy4R4pDbsZhp0i6BsA)  
摘要：云渲染就是依托于云计算的一种云端渲染服务。用户将本地的渲染任务提交到远程服务器，通过远程的计算机集群资源进行运算操作，将上传的任务进行云端处理后再返回给本地，由用户下载提取。使用云端渲染可以更好的支持业务需求，如随时支持特效相关的H5活动，以及充分利用服务端的大内存和高性能，结合Y-tech的AI能力，给用户提供更完美好看、好玩的画面效果。  
主要的难点之一便是云端渲染的耗时优化，随着输入视频时长的增加以及业务场景复杂度的提升，耗时的增加导致用户的等待时长以及服务器资源的成本都在不断提高，所以云端渲染急需在性能耗时等方面做各种优化处理。  
通过充分利用服务器的多核优势：将解码、渲染、编码三个模块解耦拆分、进行多线程异步处理。针对一些长视频处理过程中的瓶颈，进一步将编码、解码模块内部重构，支持多线程解码、多线程编码，在性能上满足更复杂的需求。使用一分钟的输入视频进行测试，特效及输出不变的情况下，针对多线程方面的优化耗时提升明显，优化后CPU和GPU利用率是之前的三倍，耗时较之前可提高2.5倍左右。  
- [编译器与IR的思考: LLVM IR，SPIR-V到MLIR | StarryHeavensAbove](https://mp.weixin.qq.com/s/G36IllLOTXXbc4LagbNH9Q)  
原文：https://www.lei.chat/posts/compilers-and-irs-llvm-ir-spirv-and-mlir/  
摘要：本篇试图记录作者对编译器以及中间表示发展的历史和趋势的理解，为以后探讨具体的编译器以及中间表示机制奠定基础。  
总结一下：人类通过抽象来应对复杂性，编译器是能够自动转换抽象级别的效率工具，首要考虑正确性，其次产生高效代码。正确性需要明确的语义以及各种验证来保障。编译器产生的代码可以给我们绝大部分的性能， 这可以让我们把工程力量集中到真正核心的对性能影响最大的部分。  
LLVM 通过其 LLVM IR 和库解耦并模块化了编译器，同时用文本表示将 UNIX 哲学带入了编译器。但 LLVM 的设计折中意味着其并不适合所有的领域，比如 LLVM 并不对稳定性和兼容性提供强保障，LLVM IR 本身也是不可分割的中心化中间表示；SPIR-V 是着眼于 GPU 领域的行业标准规范，其通过技术上的机制和组织上的流程来维持其可扩展性。SPIR-V 有着稳定的字节码和兼容性保障；MLIR 进一步解耦了中间表示；完整的中间表示被分割成可以按需选取的 dialect；其基础设施极大地简化定义各种层级 operation 和转换。这符合技术发展的趋势——技术一般都是从单一的强耦合的解决方案渐渐演进到多种多样的可定制化的解决方案。将来开发领域专用编译器可能真的只是选取、定制、以及混合各种 dialect 这般简单。  
- [GPU架构变迁之AI系统视角：从费米到安培 | 杨军](https://mp.weixin.qq.com/s/ICiOMXMDLzZ_TupfY4kEhQ)  
摘要：本文以AI系统视角为切入点，从最早应用于深度学习计算加速的GTX 580开始，回顾了从Fermi到Ampere共7代架构的5个部分（单设备硬件架构、跨设备硬件架构、AI workload及应用场景的演进、AI软件栈的演进、生态发展），期望随着未来Ampere-Next以及Ampere-Next-Next的发布，一起经历见证AI系统领域和NV GPU架构的共同演进发展。  
