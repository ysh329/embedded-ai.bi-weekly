---
layout: default
---

# 嵌入式AI简报 (2022-02-22)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  


> 导读：


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：




> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [Android 12 正式发布 | 开发者们的全新舞台 | Android 开发者](https://mp.weixin.qq.com/s/oQr0cGxi8SbjahmUWCtPtg)  
摘要：10 月 4 日，我们已经将源代码推送至 Android 开源项目 (AOSP)，并正式发布最新版本的 Android。Android 12 会在接下来的几周内推送至 Pixel 设备，并在今年晚些时候覆盖三星 Galaxy、一加、OPPO、realme、TECNO、Vivo 和小米设备。  
性能表现：更快、更高效的系统性能 - 我们将核心系统服务所需的 CPU 时间减少了 22%，并将对大核的使用减少了 15%。我们还改善了应用的启动时间，并优化了 I/O 以加快应用的加载速度；对于数据库查询，在使用 CursorWindow 处理大量数据的时候，我们将其性能提高了 49 倍之多。
更快的机器学习 - Android 12 能帮助您充分利用 ML 加速器，并通过 Neural Networks API 始终获得最佳的性能表现。ML 加速器驱动现在也可以独立于平台版本之外，通过 Google Play 服务进行更新，因此您可以在任何兼容的设备上使用最新的驱动。  


## 论文  


- [英特尔用ViT做密集预测效果超越卷积，性能提高28%，mIoU直达SOTA｜在线可玩 | 量子位](https://mp.weixin.qq.com/s/GDFyV_QazG8z36y6BO6iQg)  
文章：https://arxiv.org/abs/2103.13413  
模型：https://github.com/intel-isl/dpt  
在线：https://huggingface.co/spaces/akhaliq/DPT-Large  
摘要：英特尔最近用Vision Transformer搞了一个密集预测模型，结果是相比全卷积，该模型在单目深度估计应用任务上，性能提高了28%。在语义分割任务上，该模型更是在ADE20K数据集上以49.02%的mIoU创造了新的SOTA。  
此模型名叫DPT，也就是dense prediction transformer的简称。总的来说，DPT沿用了在卷积网络中常用的编码器-解码器结构，主要是在编码器的基础计算构建块用了transformer。它通过利用ViT为主干，将ViT提供的词包（bag-of-words）重新组合成不同分辨率的图像特征表示，然后使用卷积解码器将该表示逐步组合到最终的密集预测结果。  



## 开源项目




## 博文



- [MegEngine 的 CUDA 矩阵乘法终极优化 | 旷视研究院](https://mp.weixin.qq.com/s/XX5q36gwfqKyPaQOkiUx8w)  
摘要：单精度矩阵乘法（SGEMM）几乎是每一位学习 CUDA 的同学绕不开的案例，这个经典的计算密集型案例可以很好地展示 GPU 编程中常用的优化技巧，而能否写出高效率的 SGEMM Kernel ，也是反应每一位 CUDA 程序员对 GPU 体系结构的理解程度的优秀考题。本文将详细介绍 CUDA SGEMM 的优化手段，适合认真阅读过《CUDA C++Programming Guide》，具备一定 CUDA 编程基础的同学阅读，希望能给追求极致性能的同学们一些启发。







